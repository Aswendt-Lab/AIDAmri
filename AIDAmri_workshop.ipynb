{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [Before we start](#intro)\n",
    "    - [Prerequisites](#prereq)\n",
    "    - [About the notebook](#aboutnb)\n",
    "    - [Test data](#testd)\n",
    "* [From build to launch](#fbtl)\n",
    "    - [AIDAmri image build](#build)\n",
    "    - [Create a container](#contcreate)\n",
    "    - [(Re-)start the container](#contstart)\n",
    "    - [Basic usage](#usage)\n",
    "* [Data processing with AIDAmri](#proc)\n",
    "    - [T2w](#t2w)\n",
    "    - [DTI](#dti)\n",
    "    - [fMRI](#frmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we start <a class=\"anchor\" id=\"intro\"></a>\n",
    "## Prerequisites <a class=\"anchor\" id=\"prereq\"></a>\n",
    "The following programs are **necessary** or recommended to use AIDAmri:\n",
    "* **Docker**\n",
    "* **ca. 12 GB free memory**\n",
    "* **<span style=color:orange;>For Windows users:</span> Bash terminal or Linux subsystem** (we recommend [git BASH](https://gitforwindows.org/))\n",
    "* Anaconda Navigator or local Jupyter notebook installment to execute the code cells directly in this script\n",
    "\n",
    "## About the notebook <a class=\"anchor\" id=\"aboutnb\"></a>\n",
    "You will notice that the code lines are mostly prefaced by an `!`. This designates the command to run on Bash, i.e. the Unix-shell system. TComments in code cells are prefaced by `#`.\n",
    ">&#128681; A red flag will be used, whenever a difference is noted between the notebook and working in an interactive terminal like the Linux terminal or git BASH. There, the `!` is not necessary when typing in a command.\n",
    "\n",
    "<span style=color:orange;font-weight:bold;>For Windows users:</span> Windows does not use the Bash shell per default but the command line prompt (CMD). If you opened this notebook via the Anaconda Navigator, open git BASH or any other Bash terminal available and type in `. PATH\\TO\\ANACONDA\\Scripts\\activate` (replace `\\PATH\\TO\\ANACONDA` with your Anaconda install path) to let the terminal access and run the `jupyter notebook` command to re-open Jupyter, so the script uses the Bash shell instead of CMD.\n",
    "\n",
    "## Test data <a class=\"anchor\" id=\"testd\"></a>\n",
    "We provide test data for the purpose of this notebook [here](https://gin.g-node.org/pallastn/AIDA_dataset). Download the `testData.zip` and unpack it. Within the new folder, another folder named `testData`. Move this folder into the same folder where this notebook is located. The data is acquired with Bruker 9.4T - cryo coil setup: adult C57BL7/6 mouse, T2-weighted (anatomical scan), DTI (structural connectivity scan), rs-fMRI (functional connectivity scan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From build to launch <a class=\"anchor\" id=\"fbtl\"></a>\n",
    "## AIDAmri image build<a class=\"anchor\" id=\"build\"></a>\n",
    "We provide a Dockerfile in GitHub that functions as a protocol to assemble an image. This image is the installation of AIDAmri as well as the environment to run AIDAmri commands and is based on Linux Ubuntu 18.04. To initiate the build, please check first if you are within the repository directory using `pwd`. It should end with `AIDAmri`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/Documents/AIDA/AIDAmri\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If right, also check whether the Dockerfile is present, as well as the `bin` and `lib` directory. Also, the testData folder should be present for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIDA_Logo.png\r\n",
      "aidamri_v1.1.yml\r\n",
      "AIDAmri_workshop.ipynb\r\n",
      "ARA\r\n",
      "bin\r\n",
      "Dockerfile\r\n",
      "Docker_manual.pdf\r\n",
      "docker_runfile.sh\r\n",
      "fslinstaller_mod.py\r\n",
      "lib\r\n",
      "LICENSE\r\n",
      "manual.pdf\r\n",
      "niftyreg-AIDA_verified.zip\r\n",
      "README.md\r\n",
      "requirements.txt\r\n",
      "testData\r\n",
      "tools\r\n"
     ]
    }
   ],
   "source": [
    "!ls -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assuring that everything is on the right place, you may initiate the build.\n",
    "\n",
    "<span style=color:orange;font-weight:bold>For Windows and Mac:</span> The Docker engine may need to be started by opening Docker Desktop. This allows you to run the docker build command.\n",
    "\n",
    "**Issues and troubleshoot:**\n",
    "* Some Linux operating systems do not start the Docker daemon automatically. Enter `sudo systemctl start docker` in a separate terminal to start it manually.\n",
    "* FSL does not support certain CPU architecture. This may cause issues on M1/M2 macbooks. Unfortunately, a solution is still pending. We will update our software as soon as FSL architecture support is eventually expanded or we find another suitable solution.\n",
    "\n",
    "<span style=\"color:red;font-weight:bold;\">Warning: </span>The initial building process currenty is quite heavy and may take approximately 1.5 hours. Please take this in mind initiating the build. Furthermore, some of the steps currenty output a lot of warnings, causing large amounts of text and possibly some confusion. We assure you that this does not affect the AIDAmri tools and data processing in any way. We will optimize the building process and remove the source of the warning in the future. These issues will not occur when iterating the build after an update as long as the image stays installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (0/2)                                          docker:default\n",
      " => [internal] load .dockerignore                                          0.2s\n",
      " => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (0/2)                                          docker:default\n",
      " => [internal] load .dockerignore                                          0.3s\n",
      " => [internal] load build definition from Dockerfile                       0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (0/2)                                          docker:default\n",
      " => [internal] load .dockerignore                                          0.5s\n",
      " => [internal] load build definition from Dockerfile                       0.5s\n",
      "\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (0/2)                                          docker:default\n",
      " => [internal] load .dockerignore                                          0.6s\n",
      "\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load build definition from Dockerfile                       0.6s\n",
      "\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (1/2)                                          docker:default\n",
      " => [internal] load .dockerignore                                          0.8s\n",
      "\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.0s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.3s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.6s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.8s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.9s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.1s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.2s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.4s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.5s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.7s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.8s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.0s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          0.1s\n",
      "\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.1s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          0.2s\n",
      "\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.3s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          0.4s\n",
      " => => transferring context: 5.15MB                                        0.1s\n",
      "\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.4s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          0.5s\n",
      " => => transferring context: 24.31MB                                       0.3s\n",
      "\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.6s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          0.7s\n",
      " => => transferring context: 45.90MB                                       0.4s\n",
      "\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.7s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          0.8s\n",
      " => => transferring context: 89.43MB                                       0.5s\n",
      "\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.8s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          0.9s\n",
      " => => transferring context: 111.40MB                                      0.7s\n",
      "\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.0s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          1.1s\n",
      " => => transferring context: 133.93MB                                      0.8s\n",
      "\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.1s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          1.2s\n",
      " => => transferring context: 176.91MB                                      0.9s\n",
      "\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.2s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          1.3s\n",
      " => => transferring context: 197.17MB                                      1.0s\n",
      "\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.3s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          1.4s\n",
      "\u001b[34m => => transferring context: 220.83MB                                      1.1s\n",
      "\u001b[0m\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.5s (4/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m => [internal] load build context                                          1.5s\n",
      "\u001b[34m => => transferring context: 220.83MB                                      1.1s\n",
      "\u001b[0m\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.6s (5/28)                                         docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          1.6s\n",
      "\u001b[0m\u001b[34m => => transferring context: 220.83MB                                      1.1s\n",
      "\u001b[0m\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.7s (29/29)                                        docker:default\n",
      "\u001b[34m => CACHED [ 7/24] RUN python3 -m venv /opt/env                            0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/24] RUN PYTHON3 -m pip install --upgrade setuptools         0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/24] COPY requirements.txt requirements.txt                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/24] RUN pip install --upgrade pip && pip install -r requir  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [11/24] COPY fslinstaller_mod.py ./                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [12/24] RUN python3 fslinstaller_mod.py -V 5.0.11               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [13/24] RUN . /usr/local/fsl/etc/fslconf/fsl.sh                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [14/24] RUN export FSLDIR PATHs                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [15/24] RUN mkdir -p NiftyReg/niftyreg_source/                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [16/24] WORKDIR /aida/NiftyReg                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [17/24] RUN git clone git://git.code.sf.net/p/niftyreg/git nif  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [18/24] RUN export NIFTYRREG_INSTALL=../niftyreg_install        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [19/24] RUN export PATH && export LD_LIBRARY_PATH               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [20/24] WORKDIR /aida                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [21/24] RUN wget https://github.com/frankyeh/DSI-Studio/releas  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [22/24] COPY bin/ bin/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [23/24] COPY lib/ lib/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [24/24] RUN echo \"/aida/bin/dsi_studio_ubuntu_1804/dsi-studio/  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:6cea76a670c1e2a661bfa218f0f0b4434289b6266fe08  0.1s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/aidamri:latest                          0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 4.9s (29/29)                                        docker:default\n",
      "\u001b[34m => CACHED [ 7/24] RUN python3 -m venv /opt/env                            0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/24] RUN PYTHON3 -m pip install --upgrade setuptools         0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/24] COPY requirements.txt requirements.txt                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/24] RUN pip install --upgrade pip && pip install -r requir  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [11/24] COPY fslinstaller_mod.py ./                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [12/24] RUN python3 fslinstaller_mod.py -V 5.0.11               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [13/24] RUN . /usr/local/fsl/etc/fslconf/fsl.sh                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [14/24] RUN export FSLDIR PATHs                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [15/24] RUN mkdir -p NiftyReg/niftyreg_source/                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [16/24] WORKDIR /aida/NiftyReg                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [17/24] RUN git clone git://git.code.sf.net/p/niftyreg/git nif  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [18/24] RUN export NIFTYRREG_INSTALL=../niftyreg_install        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [19/24] RUN export PATH && export LD_LIBRARY_PATH               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [20/24] WORKDIR /aida                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [21/24] RUN wget https://github.com/frankyeh/DSI-Studio/releas  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [22/24] COPY bin/ bin/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [23/24] COPY lib/ lib/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [24/24] RUN echo \"/aida/bin/dsi_studio_ubuntu_1804/dsi-studio/  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:6cea76a670c1e2a661bfa218f0f0b4434289b6266fe08  0.1s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/aidamri:latest                          0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.0s (29/29)                                        docker:default\n",
      "\u001b[34m => CACHED [ 7/24] RUN python3 -m venv /opt/env                            0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/24] RUN PYTHON3 -m pip install --upgrade setuptools         0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/24] COPY requirements.txt requirements.txt                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/24] RUN pip install --upgrade pip && pip install -r requir  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [11/24] COPY fslinstaller_mod.py ./                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [12/24] RUN python3 fslinstaller_mod.py -V 5.0.11               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [13/24] RUN . /usr/local/fsl/etc/fslconf/fsl.sh                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [14/24] RUN export FSLDIR PATHs                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [15/24] RUN mkdir -p NiftyReg/niftyreg_source/                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [16/24] WORKDIR /aida/NiftyReg                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [17/24] RUN git clone git://git.code.sf.net/p/niftyreg/git nif  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [18/24] RUN export NIFTYRREG_INSTALL=../niftyreg_install        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [19/24] RUN export PATH && export LD_LIBRARY_PATH               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [20/24] WORKDIR /aida                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [21/24] RUN wget https://github.com/frankyeh/DSI-Studio/releas  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [22/24] COPY bin/ bin/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [23/24] COPY lib/ lib/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [24/24] RUN echo \"/aida/bin/dsi_studio_ubuntu_1804/dsi-studio/  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:6cea76a670c1e2a661bfa218f0f0b4434289b6266fe08  0.1s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/aidamri:latest                          0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.2s (29/29)                                        docker:default\n",
      "\u001b[34m => CACHED [ 7/24] RUN python3 -m venv /opt/env                            0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/24] RUN PYTHON3 -m pip install --upgrade setuptools         0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/24] COPY requirements.txt requirements.txt                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/24] RUN pip install --upgrade pip && pip install -r requir  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [11/24] COPY fslinstaller_mod.py ./                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [12/24] RUN python3 fslinstaller_mod.py -V 5.0.11               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [13/24] RUN . /usr/local/fsl/etc/fslconf/fsl.sh                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [14/24] RUN export FSLDIR PATHs                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [15/24] RUN mkdir -p NiftyReg/niftyreg_source/                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [16/24] WORKDIR /aida/NiftyReg                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [17/24] RUN git clone git://git.code.sf.net/p/niftyreg/git nif  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [18/24] RUN export NIFTYRREG_INSTALL=../niftyreg_install        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [19/24] RUN export PATH && export LD_LIBRARY_PATH               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [20/24] WORKDIR /aida                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [21/24] RUN wget https://github.com/frankyeh/DSI-Studio/releas  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [22/24] COPY bin/ bin/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [23/24] COPY lib/ lib/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [24/24] RUN echo \"/aida/bin/dsi_studio_ubuntu_1804/dsi-studio/  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:6cea76a670c1e2a661bfa218f0f0b4434289b6266fe08  0.1s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/aidamri:latest                          0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.3s (29/29)                                        docker:default\n",
      "\u001b[34m => CACHED [ 7/24] RUN python3 -m venv /opt/env                            0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/24] RUN PYTHON3 -m pip install --upgrade setuptools         0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/24] COPY requirements.txt requirements.txt                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/24] RUN pip install --upgrade pip && pip install -r requir  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [11/24] COPY fslinstaller_mod.py ./                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [12/24] RUN python3 fslinstaller_mod.py -V 5.0.11               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [13/24] RUN . /usr/local/fsl/etc/fslconf/fsl.sh                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [14/24] RUN export FSLDIR PATHs                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [15/24] RUN mkdir -p NiftyReg/niftyreg_source/                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [16/24] WORKDIR /aida/NiftyReg                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [17/24] RUN git clone git://git.code.sf.net/p/niftyreg/git nif  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [18/24] RUN export NIFTYRREG_INSTALL=../niftyreg_install        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [19/24] RUN export PATH && export LD_LIBRARY_PATH               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [20/24] WORKDIR /aida                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [21/24] RUN wget https://github.com/frankyeh/DSI-Studio/releas  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [22/24] COPY bin/ bin/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [23/24] COPY lib/ lib/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [24/24] RUN echo \"/aida/bin/dsi_studio_ubuntu_1804/dsi-studio/  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:6cea76a670c1e2a661bfa218f0f0b4434289b6266fe08  0.1s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/aidamri:latest                          0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.5s (29/29)                                        docker:default\n",
      "\u001b[34m => CACHED [ 7/24] RUN python3 -m venv /opt/env                            0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/24] RUN PYTHON3 -m pip install --upgrade setuptools         0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/24] COPY requirements.txt requirements.txt                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/24] RUN pip install --upgrade pip && pip install -r requir  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [11/24] COPY fslinstaller_mod.py ./                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [12/24] RUN python3 fslinstaller_mod.py -V 5.0.11               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [13/24] RUN . /usr/local/fsl/etc/fslconf/fsl.sh                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [14/24] RUN export FSLDIR PATHs                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [15/24] RUN mkdir -p NiftyReg/niftyreg_source/                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [16/24] WORKDIR /aida/NiftyReg                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [17/24] RUN git clone git://git.code.sf.net/p/niftyreg/git nif  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [18/24] RUN export NIFTYRREG_INSTALL=../niftyreg_install        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [19/24] RUN export PATH && export LD_LIBRARY_PATH               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [20/24] WORKDIR /aida                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [21/24] RUN wget https://github.com/frankyeh/DSI-Studio/releas  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [22/24] COPY bin/ bin/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [23/24] COPY lib/ lib/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [24/24] RUN echo \"/aida/bin/dsi_studio_ubuntu_1804/dsi-studio/  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:6cea76a670c1e2a661bfa218f0f0b4434289b6266fe08  0.1s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/aidamri:latest                          0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 5.5s (29/29) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.7s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            2.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          1.6s\n",
      "\u001b[0m\u001b[34m => => transferring context: 220.83MB                                      1.1s\n",
      "\u001b[0m\u001b[34m => [ 1/24] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/24] RUN apt-get update -y && apt-get upgrade -y && apt-get  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/24] RUN wget https://github.com/Kitware/CMake/releases/dow  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/24] RUN mkdir aida/                                         0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/24] WORKDIR /aida/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/24] RUN apt install -y python3 python3-pip && python3 -m p  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/24] RUN python3 -m venv /opt/env                            0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/24] RUN PYTHON3 -m pip install --upgrade setuptools         0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/24] COPY requirements.txt requirements.txt                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/24] RUN pip install --upgrade pip && pip install -r requir  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [11/24] COPY fslinstaller_mod.py ./                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [12/24] RUN python3 fslinstaller_mod.py -V 5.0.11               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [13/24] RUN . /usr/local/fsl/etc/fslconf/fsl.sh                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [14/24] RUN export FSLDIR PATHs                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [15/24] RUN mkdir -p NiftyReg/niftyreg_source/                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [16/24] WORKDIR /aida/NiftyReg                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [17/24] RUN git clone git://git.code.sf.net/p/niftyreg/git nif  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [18/24] RUN export NIFTYRREG_INSTALL=../niftyreg_install        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [19/24] RUN export PATH && export LD_LIBRARY_PATH               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [20/24] WORKDIR /aida                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [21/24] RUN wget https://github.com/frankyeh/DSI-Studio/releas  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [22/24] COPY bin/ bin/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [23/24] COPY lib/ lib/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [24/24] RUN echo \"/aida/bin/dsi_studio_ubuntu_1804/dsi-studio/  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:6cea76a670c1e2a661bfa218f0f0b4434289b6266fe08  0.1s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/aidamri:latest                          0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker build -t aidamri:latest -f Dockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the installed images in Docker desktop or by using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY    TAG       IMAGE ID       CREATED          SIZE\r\n",
      "aidamri       latest    6cea76a670c1   15 minutes ago   11.7GB\r\n",
      "hello-world   latest    9c7a54a9a43c   4 months ago     13.3kB\r\n"
     ]
    }
   ],
   "source": [
    "!docker images -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that so-called dangling images exist that might appear as artifacts of the building process. They are not named or tagged. They will not impose any issues but you can remove them with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! This will remove:\n",
      "  - all stopped containers\n",
      "  - all networks not used by at least one container\n",
      "  - all dangling images\n",
      "  - all dangling build cache\n",
      "\n",
      "Are you sure you want to continue? [y/N] Deleted build cache objects:\n",
      "ivz6oojk5sxru7n6pbg0mlc6y\n",
      "p8b36zjm3ft8fm2nwn2rgtfdk\n",
      "zgczrjfwqdincdve2d0pzidjb\n",
      "\n",
      "Total reclaimed space: 220.7MB\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes | docker system prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">&#128681; When working with an interactive terminal, you will be asked for permission to prune the system. The `yes |` part of the command pipes the directive for permission to the pruning command since the code cells here are non-interactive. You can just type in `docker system prune` when using the shell normally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a container<a class=\"anchor\" id=\"contcreate\"></a>\n",
    "A container is a runnable instance of an image, comparable to an environment. This instance needs to be initiated and run. At the same time, it is possible to mount a volume (i.e. a directory accessible from the container and from the host). If available, you can open Docker Desktop and click the \"Run\" button of the image of choice. There, you can enter the host path of your volume where you have the data stored you wish to process with AIDAmri. You also can enter a path name in the container. We recommend to name the target path like the source path to make it easier to navigate within the container. You also can initiate the container via command.\\\n",
    "<span style=color:red;font-weight:bold;>Attention:</span> Due to technical reasons, the following code line differ accordingly to the operative system. Run the cell corresponding to your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIDA_Logo.png\t\tDocker_manual.pdf    niftyreg-AIDA_verified.zip\n",
      "aidamri_v1.1.yml\tdocker_runfile.sh    README.md\n",
      "AIDAmri_workshop.ipynb\tfslinstaller_mod.py  requirements.txt\n",
      "ARA\t\t\tlib\t\t     testData\n",
      "bin\t\t\tLICENSE\t\t     tools\n",
      "Dockerfile\t\tmanual.pdf\n",
      "docker: Error response from daemon: Conflict. The container name \"/aidamri\" is already in use by container \"80868750fa2160e27a0b6f00f3f9f612d9132bbb036f0d4d62987214c658829f\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "# For Mac/Linux\n",
    "!ls\n",
    "!docker run -dit --name aidamri --mount type=bind,source=\"$(pwd)\"/testData,target=/testData aidamri:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: invalid mount config for type \"bind\": invalid mount path: '%cd%/testData' mount path must be absolute.\r\n",
      "See 'docker run --help'.\r\n"
     ]
    }
   ],
   "source": [
    "# For Windows\n",
    "!docker run -dit --name aidamri --mount type=bind,source=%cd%/testData,target=/testData aidamri:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The container will be active right away. Stopping the container will not delete it and it can be started again. Let us stop the container for a moment. We will launch it again in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aidamri\r\n"
     ]
    }
   ],
   "source": [
    "!docker stop aidamri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Re-)start the container <a class=\"anchor\" id=\"contstart\"></a>\n",
    "Let us check first, what containers are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE            COMMAND       CREATED          STATUS          PORTS     NAMES\r\n",
      "80868750fa21   aidamri:latest   \"/bin/bash\"   11 minutes ago   Up 11 minutes             aidamri\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `-a` flag is necessary to list all containers, including exited and running containers (see the `STATUS` column). The aidamri container should be there but exited.\n",
    "It is also possible to format this output (i.e. for displaying which volumes are mounted to the respective containers). See the [ps manual page](https://docs.docker.com/engine/reference/commandline/ps/) to specify your output. Use the `--no-trunc` flag to avoid clipping of the output. The following command gives you an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINERID 80868750fa2160e27a0b6f00f3f9f612d9132bbb036f0d4d62987214c658829f\r\n",
      "\tName:\taidamri\r\n",
      "\tMount:\t/home/user/Documents/AIDA/AIDAmri/testData\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps -a --no-trunc --format \"CONTAINERID {{$.ID}}\\n\\tName:\\t{{.Names}}\\n\\tMount:\\t{{.Mounts}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you already mounted a volume, you can just start the container. If you need to mount another volume, you need to run a new container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aidamri\r\n"
     ]
    }
   ],
   "source": [
    "!docker start aidamri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">&#128681; To enter a started container in a terminal, use `docker attach aidamri`. Windows and git BASH users, respectively, may need to type in `winpty docker attach aidamri` if the simple attach command does not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic usage<a class=\"anchor\" id=\"usage\"></a>\n",
    "Now, you can access the container. \n",
    "> &#128681; When using a terminal, you may input the attach command\n",
    "` docker attach aidamri` \n",
    "to activate the interactive mode. You then will notice that the preface in the terminal changed from your user name to\n",
    "```\n",
    "root@<CONTAINER ID>:/aida#\n",
    "```\n",
    "Typing in `ls -1` would result into the following output:\n",
    "```\n",
    "NiftyReg\n",
    "bin\n",
    "data\n",
    "dsi_studio_ubuntu1804\n",
    "fslinstaller_mod.py\n",
    "lib\n",
    "requirements.txt\n",
    "```\n",
    "To exit the container you are attached to, type in `exit` (keep in mind that this stops the container and you would need to restart it, when wishing to using it again. \n",
    "\n",
    "We will use `docker exec` to request an operation within the container. For the sake of readability the lines are broken by backslashes (`\\`)\n",
    "Using the `ls -1` in this way should generate same output as if we would type in the command in the terminal while attached to the container. Run the following cells for different directory lists. The `-1` flag arranges the output as a column. Later, the `-t` flag will be used instead to sort folder contents from newest to oldest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NiftyReg\r\n",
      "bin\r\n",
      "dsi_studio_ubuntu1804\r\n",
      "fslinstaller_mod.py\r\n",
      "lib\r\n",
      "requirements.txt\r\n"
     ]
    }
   ],
   "source": [
    "#directory content\n",
    "!docker exec aidamri \\\n",
    "ls -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAT_0052\r\n",
      "MAT_0017\r\n",
      "dev\r\n",
      "proc\r\n",
      "sys\r\n",
      "etc\r\n",
      "aida\r\n",
      "tmp\r\n",
      "root\r\n",
      "opt\r\n",
      "cmake-3.23.2\r\n",
      "lib\r\n",
      "bin\r\n",
      "run\r\n",
      "sbin\r\n",
      "var\r\n",
      "lib64\r\n",
      "media\r\n",
      "mnt\r\n",
      "srv\r\n",
      "usr\r\n",
      "testData\r\n",
      "boot\r\n",
      "home\r\n"
     ]
    }
   ],
   "source": [
    "#aida directory content\n",
    "!docker exec -w / aidamri \\\n",
    "ls -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3_fMRIActivity\r\n",
      "2.3_fMRIPreProcessing\r\n",
      "3.2_DTIConnectivity\r\n",
      "2.2_DTIPreProcessing\r\n",
      "2.1_T2PreProcessing\r\n",
      "conv2Nifti_auto.py\r\n",
      "groupMapping.csv\r\n",
      "stat_result.json\r\n",
      "4.1_ROI_analysis\r\n",
      "AIDA_gui.py\r\n",
      "AIDA_gui_support.py\r\n",
      "batchProc.py\r\n",
      "dsi_studio_ubuntu_1804\r\n",
      "3.1_T2Processing\r\n",
      "3.2.1_DTIdata_extract\r\n",
      "1_PV2NIfTiConverter\r\n"
     ]
    }
   ],
   "source": [
    "#bin directory content\n",
    "!docker exec -w /aida/bin aidamri \\\n",
    "ls -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `-w` flag was used to set the working directories to `/` and `/aida/bin`, respectively. The former directory is the parental or root directory of the file system in the container. It contains our testData folder, as well as a `bin` folder. Keep in mind that this `bin` folder is **not** the directory containing the AIDAmri tools. Those are located in the `/aida/bin` directory.\n",
    "\n",
    "> &#128681; When working in an interactive terminal, you need to type in the change directory command (`cd PATH/TO/FOLDER`) to set your working directory. You then can type in the second command, e.g. `ls -1`.\n",
    "\n",
    "In the first list no working directory was specified. In this case, the default directory was targeted, called `aida`. You can check the working directory by the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aida\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec aidamri \\\n",
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing with AIDAmri<a class=\"anchor\" id=\"proc\"></a>\n",
    "## T2w<a class=\"anchor\" id=\"t2w\"></a>\n",
    "Starting with pre-processing the T2w single file test data, first check if the data is complete. Given are two NIfTI files, the 5.1 test data, i.e. the base Nifty file containing the brain image, as well as a stroke mask, segmenting the lesion present in said image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stroke_mask.nii.gz\r\n",
      "testData.5.1.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/T2w aidamri ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the pre-processing script. Here, the image will be re-orientated from head supine to prone. The view direction is rostrad, i.e. the right side of the mouse at the right side at image layers of the coronal plane. Also, bias-field correction and brain extraction is performed in this process.\n",
    "Set the working directory to the 2.1_T2PreProcessing folder. Alternatively, pass the full path to the python command directly (`python /aida/bin/2.1_T2PreProcessing/preProcessing_T2.py...`). This alternative is viable for every provided script.\n",
    "This process might take a while. The output of the process will be visible in real-time when running the script in an interactive shell. Here, it may not be visible directy, but you can see the changes in the T2w folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0% |                                                                        |\r",
      "  2% |#                                                                       |\r",
      "  4% |###                                                                     |\r",
      "  6% |####                                                                    |\r",
      "  8% |######                                                                  |\r",
      " 10% |#######                                                                 |\r",
      " 12% |#########                                                               |\r",
      " 14% |##########                                                              |\r",
      " 16% |############                                                            |\r",
      " 18% |#############                                                           |\r",
      " 20% |###############                                                         |\r",
      " 22% |################                                                        |\r",
      " 25% |##################                                                      |\r",
      " 27% |###################                                                     |\r",
      " 29% |#####################                                                   |\r",
      " 31% |######################                                                  |\r",
      " 33% |########################                                                |\r",
      " 35% |#########################                                               |\r",
      " 37% |###########################                                             |\r",
      " 39% |############################                                            |\r",
      " 41% |##############################                                          |\r",
      " 43% |###############################                                         |\r",
      " 45% |#################################                                       |\r",
      " 47% |##################################                                      |\r",
      " 50% |####################################                                    |\r",
      " 52% |#####################################                                   |\r",
      " 54% |#######################################                                 |\r",
      " 56% |########################################                                |\r",
      " 58% |##########################################                              |\r",
      " 60% |###########################################                             |\r",
      " 62% |#############################################                           |\r",
      " 64% |##############################################                          |\r",
      " 66% |################################################                        |\r",
      " 68% |#################################################                       |\r",
      " 70% |###################################################                     |\r",
      " 72% |####################################################                    |\r",
      " 75% |######################################################                  |\r",
      " 77% |#######################################################                 |\r",
      " 79% |#########################################################               |\r",
      " 81% |##########################################################              |\r",
      " 83% |############################################################            |\r",
      " 85% |#############################################################           |\r",
      " 87% |###############################################################         |\r",
      " 89% |################################################################        |\r",
      " 91% |##################################################################      |\r",
      " 93% |###################################################################     |\r",
      " 95% |#####################################################################   |\r",
      " 97% |######################################################################  |\r",
      "100% |########################################################################|\r",
      "\r\n",
      "T2 Preprocessing  \u001b[5m...\u001b[0m (wait!)\r",
      "T2 Preprocessing  \u001b[0;30;42m COMPLETED \u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /aida/bin/2.1_T2PreProcessing aidamri \\\n",
    "python preProcessing_T2.py -i /testData/T2w/testData.5.1.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should show that the process completed. The progress bar is a residual of the proceding output normally displayed in the shell.\n",
    "Again, check the contents of the T2w folder. Apart from the already present files, it should include the following:\n",
    "* testDataBias.nii.gz (Bias field file)\n",
    "* testDataBiasBet.nii.gz (Extracted brain file)\n",
    "* testDataBiasBet_mask.nii.gz (Extracted brain mask)\n",
    "* preprocess.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stroke_mask.nii.gz\r\n",
      "preprocess.log\r\n",
      "testData.5.1.nii.gz\r\n",
      "testDataBias.nii.gz\r\n",
      "testDataBiasBet.nii.gz\r\n",
      "testDataBiasBet_mask.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/T2w aidamri \\\n",
    "ls -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the registration function will be invoked. It requires the brain extracted file (`...BiasBet.nii.gz`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NiftyReg ALADIN] Command line:\n",
      "\t reg_aladin -ref /testData/T2w/testDataBiasBet.nii.gz -flo /aida/lib/NP_template_sc0.nii.gz -res /testData/T2w/testDataBiasBet_TemplateAff.nii.gz -aff /testData/T2w/testDataBiasBetMatrixAff.txt\n",
      "\n",
      "[reg_aladin_sym] Parameters\n",
      "[reg_aladin_sym] Reference image name: /testData/T2w/testDataBiasBet.nii.gz\n",
      "[reg_aladin_sym] \t256x256x48 voxels\n",
      "[reg_aladin_sym] \t0.0683594x0.0683594x0.3 mm\n",
      "[reg_aladin_sym] Floating image name: /aida/lib/NP_template_sc0.nii.gz\n",
      "[reg_aladin_sym] \t228x160x264 voxels\n",
      "[reg_aladin_sym] \t0.05x0.05x0.05 mm\n",
      "[reg_aladin_sym] Maximum iteration number: 5 (10 during the first level)\n",
      "[reg_aladin_sym] Percentage of blocks: 50 %\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Current level 1 / 3\n",
      "[reg_aladin_sym] reference image size: \t64x64x48 voxels\t0.273438x0.273438x0.3 mm\n",
      "[reg_aladin_sym] floating image size: \t57x40x66 voxels\t0.2x0.2x0.2 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [16 16 12]\n",
      "[reg_aladin_sym] Backward Block number = [15 10 17]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "1\t0\t0\t-8.725\n",
      "0\t1\t0\t-11.325\n",
      "0\t0\t1\t-4.575\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "1\t0\t0\t8.725\n",
      "0\t1\t0\t11.325\n",
      "0\t0\t1\t4.575\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "1.02586\t0.0164464\t0.0392102\t-9.243\n",
      "-0.0197475\t1.12116\t-0.00452693\t-12.2456\n",
      "-0.0443454\t0.031192\t0.948056\t-4.08053\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.972794\t-0.0131489\t-0.0402961\t8.66609\n",
      "0.0173157\t0.89158\t0.00354111\t11.0924\n",
      "0.0449328\t-0.0299489\t1.05279\t4.34451\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[reg_aladin_sym] Current level 2 / 3\n",
      "[reg_aladin_sym] reference image size: \t128x128x48 voxels\t0.136719x0.136719x0.3 mm\n",
      "[reg_aladin_sym] floating image size: \t114x80x132 voxels\t0.1x0.1x0.1 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [32 32 12]\n",
      "[reg_aladin_sym] Backward Block number = [29 20 33]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "1.02586\t0.0164464\t0.0392102\t-9.243\n",
      "-0.0197475\t1.12116\t-0.00452693\t-12.2456\n",
      "-0.0443454\t0.031192\t0.948056\t-4.08053\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "0.972794\t-0.0131489\t-0.0402961\t8.66609\n",
      "0.0173157\t0.89158\t0.00354111\t11.0924\n",
      "0.0449328\t-0.0299489\t1.05279\t4.34451\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "1.02483\t0.0241717\t0.0185933\t-9.21405\n",
      "-0.00707995\t1.13079\t-0.0161562\t-12.3742\n",
      "-0.0371059\t0.034859\t0.958463\t-4.19251\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.974937\t-0.0202467\t-0.0192542\t8.65186\n",
      "0.00663995\t0.883739\t0.0147678\t11.0587\n",
      "0.0375022\t-0.0329251\t1.04205\t4.30694\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[reg_aladin_sym] Current level 3 / 3\n",
      "[reg_aladin_sym] reference image size: \t256x256x48 voxels\t0.0683594x0.0683594x0.3 mm\n",
      "[reg_aladin_sym] floating image size: \t228x160x264 voxels\t0.05x0.05x0.05 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [64 64 12]\n",
      "[reg_aladin_sym] Backward Block number = [57 40 66]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "1.02483\t0.0241717\t0.0185933\t-9.21405\n",
      "-0.00707995\t1.13079\t-0.0161562\t-12.3742\n",
      "-0.0371059\t0.034859\t0.958463\t-4.19251\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "0.974937\t-0.0202467\t-0.0192542\t8.65186\n",
      "0.00663995\t0.883739\t0.0147678\t11.0587\n",
      "0.0375022\t-0.0329251\t1.04205\t4.30694\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "1.0258\t0.0270858\t0.021652\t-9.26662\n",
      "-0.0150526\t1.12443\t-0.0192043\t-12.2354\n",
      "-0.0363029\t0.0430287\t0.954655\t-4.24792\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.973719\t-0.022593\t-0.0225388\t8.65091\n",
      "0.0136569\t0.88834\t0.0175605\t11.0703\n",
      "0.0364123\t-0.0408989\t1.04585\t4.27969\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "Registration Performed in 0 min 49 sec\n",
      "Have a good day !\n",
      "\n",
      "[NiftyReg ALADIN] Command line:\n",
      "\t reg_aladin -ref /aida/lib/average_template_50.nii.gz -flo /testData/T2w/testDataBiasBet.nii.gz -res /testData/T2w/testDataBiasBet_IncidenceData.nii.gz -aff /testData/T2w/testDataBiasBetMatrixInv.txt\n",
      "\n",
      "[reg_aladin_sym] Parameters\n",
      "[reg_aladin_sym] Reference image name: /aida/lib/average_template_50.nii.gz\n",
      "[reg_aladin_sym] \t228x160x264 voxels\n",
      "[reg_aladin_sym] \t0.05x0.05x0.05 mm\n",
      "[reg_aladin_sym] Floating image name: /testData/T2w/testDataBiasBet.nii.gz\n",
      "[reg_aladin_sym] \t256x256x48 voxels\n",
      "[reg_aladin_sym] \t0.0683594x0.0683594x0.3 mm\n",
      "[reg_aladin_sym] Maximum iteration number: 5 (10 during the first level)\n",
      "[reg_aladin_sym] Percentage of blocks: 50 %\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Current level 1 / 3\n",
      "[reg_aladin_sym] reference image size: \t57x40x66 voxels\t0.2x0.2x0.2 mm\n",
      "[reg_aladin_sym] floating image size: \t64x64x48 voxels\t0.273438x0.273438x0.3 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [15 10 17]\n",
      "[reg_aladin_sym] Backward Block number = [16 16 12]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "1\t0\t0\t8.725\n",
      "0\t1\t0\t11.325\n",
      "0\t0\t1\t4.575\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "1\t0\t0\t-8.725\n",
      "0\t1\t0\t-11.325\n",
      "0\t0\t1\t-4.575\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "1.00601\t-0.00649697\t-0.0279012\t8.6818\n",
      "0.0089154\t0.926511\t0.00463073\t11.2134\n",
      "0.0234342\t-0.0256979\t1.06076\t4.3197\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.993351\t0.00768943\t0.0260945\t-8.82302\n",
      "-0.00944774\t1.07911\t-0.00495935\t-11.9971\n",
      "-0.0221738\t0.0259725\t0.942021\t-4.16798\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[reg_aladin_sym] Current level 2 / 3\n",
      "[reg_aladin_sym] reference image size: \t114x80x132 voxels\t0.1x0.1x0.1 mm\n",
      "[reg_aladin_sym] floating image size: \t128x128x48 voxels\t0.136719x0.136719x0.3 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [29 20 33]\n",
      "[reg_aladin_sym] Backward Block number = [32 32 12]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "1.00601\t-0.00649697\t-0.0279012\t8.6818\n",
      "0.0089154\t0.926511\t0.00463073\t11.2134\n",
      "0.0234342\t-0.0256979\t1.06076\t4.3197\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "0.993351\t0.00768943\t0.0260945\t-8.82302\n",
      "-0.00944774\t1.07911\t-0.00495935\t-11.9971\n",
      "-0.0221738\t0.0259725\t0.942021\t-4.16798\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "1.00211\t-0.00624942\t-0.0241245\t8.68661\n",
      "0.0196717\t0.929796\t0.0157848\t11.2179\n",
      "0.0266633\t-0.0358128\t1.03354\t4.27152\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.99713\t0.00759398\t0.0231585\t-8.84578\n",
      "-0.0206474\t1.07472\t-0.0168955\t-11.8045\n",
      "-0.0264393\t0.0370434\t0.966361\t-4.31371\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[reg_aladin_sym] Current level 3 / 3\n",
      "[reg_aladin_sym] reference image size: \t228x160x264 voxels\t0.05x0.05x0.05 mm\n",
      "[reg_aladin_sym] floating image size: \t256x256x48 voxels\t0.0683594x0.0683594x0.3 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [57 40 66]\n",
      "[reg_aladin_sym] Backward Block number = [64 64 12]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "1.00211\t-0.00624942\t-0.0241245\t8.68661\n",
      "0.0196717\t0.929796\t0.0157848\t11.2179\n",
      "0.0266633\t-0.0358128\t1.03354\t4.27152\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "0.99713\t0.00759398\t0.0231585\t-8.84578\n",
      "-0.0206474\t1.07472\t-0.0168955\t-11.8045\n",
      "-0.0264393\t0.0370434\t0.966361\t-4.31371\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "0.993204\t-0.0120307\t-0.0282392\t8.68278\n",
      "0.0145397\t0.917878\t0.0237863\t11.1678\n",
      "0.0349271\t-0.0285382\t1.03592\t4.29998\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "1.00568\t0.0140239\t0.027093\t-9.00525\n",
      "-0.0150411\t1.08848\t-0.0254032\t-11.9161\n",
      "-0.034322\t0.0295134\t0.963712\t-4.17553\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "Registration Performed in 0 min 43 sec\n",
      "Have a good day !\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /aida/lib/average_template_50.nii.gz -flo /testData/T2w/Stroke_mask.nii.gz -trans /testData/T2w/testDataBiasBetMatrixInv.txt -res /testData/T2w/testDataBiasBet_IncidenceData_mask.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /aida/lib/average_template_50.nii.gz\n",
      "\t228x160x264 voxels, 1 volumes\n",
      "\t0.05x0.05x0.05 mm\n",
      "Floating image name: /testData/T2w/Stroke_mask.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/T2w/testDataBiasBet_IncidenceData_mask.nii.gz\n",
      "\n",
      "[NiftyReg F3D] Command line:\n",
      "\t reg_f3d -ref /testData/T2w/testDataBiasBet.nii.gz -flo /aida/lib/NP_template_sc0.nii.gz -sx 3 -sy 3 -sz 3 -jl 0.3 -res /testData/T2w/testDataBiasBet_Template.nii.gz -cpp /testData/T2w/testDataBiasBetMatrixBspline.nii -aff /testData/T2w/testDataBiasBetMatrixAff.txt\n",
      "\n",
      "[NiftyReg F3D] OpenMP is used with 8 thread(s)\n",
      "[NiftyReg F3D] **************************************************\n",
      "[NiftyReg F3D] INPUT PARAMETERS\n",
      "[NiftyReg F3D] **************************************************\n",
      "[NiftyReg F3D] Reference image:\n",
      "[NiftyReg F3D] \t* name: /testData/T2w/testDataBiasBet.nii.gz\n",
      "[NiftyReg F3D] \t* image dimension: 256 x 256 x 48 x 1\n",
      "[NiftyReg F3D] \t* image spacing: 0.0683594 x 0.0683594 x 0.3 mm\n",
      "[NiftyReg F3D] \t* intensity threshold for timepoint 1/1: [-3.4e+38 3.4e+38]\n",
      "[NiftyReg F3D] \t* gaussian smoothing sigma: 0\n",
      "[NiftyReg F3D]\n",
      "[NiftyReg F3D] Floating image:\n",
      "[NiftyReg F3D] \t* name: /aida/lib/NP_template_sc0.nii.gz\n",
      "[NiftyReg F3D] \t* image dimension: 228 x 160 x 264 x 1\n",
      "[NiftyReg F3D] \t* image spacing: 0.05 x 0.05 x 0.05 mm\n",
      "[NiftyReg F3D] \t* intensity threshold for timepoint 1/1: [-3.4e+38 3.4e+38]\n",
      "[NiftyReg F3D] \t* gaussian smoothing sigma: 0\n",
      "[NiftyReg F3D]\n",
      "[NiftyReg F3D] Warped image padding value: nan\n",
      "[NiftyReg F3D]\n",
      "[NiftyReg F3D] Level number: 3\n",
      "[NiftyReg F3D]\n",
      "[NiftyReg F3D] Maximum iteration number per level: 300\n",
      "[NiftyReg F3D]\n",
      "[NiftyReg F3D] Final spacing in mm: 3 3 3\n",
      "[NiftyReg F3D]\n",
      "[NiftyReg F3D] The NMI is used as a similarity measure.\n",
      "[NiftyReg F3D] Similarity measure term weight: 0.695\n",
      "[NiftyReg F3D]\n",
      "[NiftyReg F3D] Bending energy penalty term weight: 0.005\n",
      "[NiftyReg F3D]\n",
      "[NiftyReg F3D] Linear energy penalty term weights: 0 0\n",
      "[NiftyReg F3D]\n",
      "[NiftyReg F3D] L2 norm of the displacement penalty term weights: 0\n",
      "[NiftyReg F3D]\n",
      "[NiftyReg F3D] Jacobian-based penalty term weight: 0.3\n",
      "[NiftyReg F3D] \t* Jacobian-based penalty term is approximated\n",
      "[NiftyReg F3D] **************************************************\n",
      "[NiftyReg F3D] Current level: 1 / 3\n",
      "[NiftyReg F3D] Current reference image\n",
      "[NiftyReg F3D] \t* image dimension: 64 x 64 x 48 x 1\n",
      "[NiftyReg F3D] \t* image spacing: 0.273438 x 0.273438 x 0.3 mm\n",
      "[NiftyReg F3D] Current floating image\n",
      "[NiftyReg F3D] \t* image dimension: 57 x 40 x 66 x 1\n",
      "[NiftyReg F3D] \t* image spacing: 0.2 x 0.2 x 0.2 mm\n",
      "[NiftyReg F3D] Current control point image\n",
      "[NiftyReg F3D] \t* image dimension: 5 x 5 x 5\n",
      "[NiftyReg F3D] \t* image spacing: 12 x 12 x 12 mm\n",
      "[NiftyReg F3D] Initial objective function: 0.782466 = (wSIM)0.785363 - (wBE)1.62589e-15 - (wLE)0 - (wL2)0 - (wJAC)0.00289714\n",
      "[NiftyReg F3D] [9] Current objective function: 0.782745 = (wSIM)0.785595 - (wBE)7.27e-08 - (wJAC)2.85e-03 [+ 0.3 mm]\n",
      "[NiftyReg F3D] [17] Current objective function: 0.782745 = (wSIM)0.785595 - (wBE)7.27e-08 - (wJAC)2.85e-03 [+ 0 mm]\n",
      "[NiftyReg F3D] Current registration level done\n",
      "[NiftyReg F3D] --------------------------------------------------\n",
      "[NiftyReg F3D] **************************************************\n",
      "[NiftyReg F3D] Current level: 2 / 3\n",
      "[NiftyReg F3D] Current reference image\n",
      "[NiftyReg F3D] \t* image dimension: 128 x 128 x 48 x 1\n",
      "[NiftyReg F3D] \t* image spacing: 0.136719 x 0.136719 x 0.3 mm\n",
      "[NiftyReg F3D] Current floating image\n",
      "[NiftyReg F3D] \t* image dimension: 114 x 80 x 132 x 1\n",
      "[NiftyReg F3D] \t* image spacing: 0.1 x 0.1 x 0.1 mm\n",
      "[NiftyReg F3D] Current control point image\n",
      "[NiftyReg F3D] \t* image dimension: 6 x 6 x 6\n",
      "[NiftyReg F3D] \t* image spacing: 6 x 6 x 6 mm\n",
      "[NiftyReg F3D] Initial objective function: 0.778444 = (wSIM)0.781265 - (wBE)1.40679e-07 - (wLE)0 - (wL2)0 - (wJAC)0.00282145\n",
      "[NiftyReg F3D] [9] Current objective function: 0.78145 = (wSIM)0.784275 - (wBE)7.48e-07 - (wJAC)2.82e-03 [+ 0.3 mm]\n",
      "[NiftyReg F3D] [17] Current objective function: 0.78145 = (wSIM)0.784275 - (wBE)7.48e-07 - (wJAC)2.82e-03 [+ 0 mm]\n",
      "[NiftyReg F3D] Current registration level done\n",
      "[NiftyReg F3D] --------------------------------------------------\n",
      "[NiftyReg F3D] **************************************************\n",
      "[NiftyReg F3D] Current level: 3 / 3\n",
      "[NiftyReg F3D] Current reference image\n",
      "[NiftyReg F3D] \t* image dimension: 256 x 256 x 48 x 1\n",
      "[NiftyReg F3D] \t* image spacing: 0.0683594 x 0.0683594 x 0.3 mm\n",
      "[NiftyReg F3D] Current floating image\n",
      "[NiftyReg F3D] \t* image dimension: 228 x 160 x 264 x 1\n",
      "[NiftyReg F3D] \t* image spacing: 0.05 x 0.05 x 0.05 mm\n",
      "[NiftyReg F3D] Current control point image\n",
      "[NiftyReg F3D] \t* image dimension: 9 x 9 x 8\n",
      "[NiftyReg F3D] \t* image spacing: 3 x 3 x 3 mm\n",
      "[NiftyReg F3D] Initial objective function: 0.776861 = (wSIM)0.779707 - (wBE)1.21322e-06 - (wLE)0 - (wL2)0 - (wJAC)0.00284508\n",
      "[NiftyReg F3D] [9] Current objective function: 0.777912 = (wSIM)0.780774 - (wBE)8.74e-06 - (wJAC)2.85e-03 [+ 0.3 mm]\n",
      "[NiftyReg F3D] [18] Current objective function: 0.778025 = (wSIM)0.780923 - (wBE)2.76e-05 - (wJAC)2.87e-03 [+ 0.3 mm]\n",
      "[NiftyReg F3D] [27] Current objective function: 0.780913 = (wSIM)0.78387 - (wBE)5.86e-05 - (wJAC)2.90e-03 [+ 0.3 mm]\n",
      "[NiftyReg F3D] [36] Current objective function: 0.78097 = (wSIM)0.783961 - (wBE)7.48e-05 - (wJAC)2.92e-03 [+ 0.075 mm]\n",
      "[NiftyReg F3D] [46] Current objective function: 0.781769 = (wSIM)0.784751 - (wBE)7.70e-05 - (wJAC)2.90e-03 [+ 0.202875 mm]\n",
      "[NiftyReg F3D] [56] Current objective function: 0.781874 = (wSIM)0.784853 - (wBE)8.00e-05 - (wJAC)2.90e-03 [+ 0.0323332 mm]\n",
      "[NiftyReg F3D] [64] Current objective function: 0.782299 = (wSIM)0.785275 - (wBE)8.01e-05 - (wJAC)2.90e-03 [+ 0.0874613 mm]\n",
      "[NiftyReg F3D] [72] Current objective function: 0.782318 = (wSIM)0.785294 - (wBE)8.24e-05 - (wJAC)2.89e-03 [+ 0.0218653 mm]\n",
      "[NiftyReg F3D] [80] Current objective function: 0.782465 = (wSIM)0.785441 - (wBE)8.50e-05 - (wJAC)2.89e-03 [+ 0.0723742 mm]\n",
      "[NiftyReg F3D] [87] Current objective function: 0.782472 = (wSIM)0.785447 - (wBE)8.69e-05 - (wJAC)2.89e-03 [+ 0.00904678 mm]\n",
      "[NiftyReg F3D] [95] Current objective function: 0.782579 = (wSIM)0.785552 - (wBE)8.81e-05 - (wJAC)2.88e-03 [+ 0.0419861 mm]\n",
      "[NiftyReg F3D] [101] Current objective function: 0.782606 = (wSIM)0.78558 - (wBE)9.20e-05 - (wJAC)2.88e-03 [+ 0.0209931 mm]\n",
      "[NiftyReg F3D] [108] Current objective function: 0.782698 = (wSIM)0.785672 - (wBE)9.30e-05 - (wJAC)2.88e-03 [+ 0.0440854 mm]\n",
      "[NiftyReg F3D] [115] Current objective function: 0.782708 = (wSIM)0.785684 - (wBE)9.97e-05 - (wJAC)2.88e-03 [+ 0.0440854 mm]\n",
      "[NiftyReg F3D] [123] Current objective function: 0.782894 = (wSIM)0.785872 - (wBE)1.08e-04 - (wJAC)2.87e-03 [+ 0.0925794 mm]\n",
      "[NiftyReg F3D] [131] Current objective function: 0.782943 = (wSIM)0.78593 - (wBE)1.21e-04 - (wJAC)2.87e-03 [+ 0.0462897 mm]\n",
      "[NiftyReg F3D] [139] Current objective function: 0.783085 = (wSIM)0.786073 - (wBE)1.21e-04 - (wJAC)2.87e-03 [+ 0.0590193 mm]\n",
      "[NiftyReg F3D] [146] Current objective function: 0.783086 = (wSIM)0.78608 - (wBE)1.29e-04 - (wJAC)2.86e-03 [+ 0.0590193 mm]\n",
      "[NiftyReg F3D] [156] Current objective function: 0.783393 = (wSIM)0.786416 - (wBE)1.61e-04 - (wJAC)2.86e-03 [+ 0.169467 mm]\n",
      "[NiftyReg F3D] [164] Current objective function: 0.783458 = (wSIM)0.786691 - (wBE)3.13e-04 - (wJAC)2.92e-03 [+ 0.169467 mm]\n",
      "[NiftyReg F3D] [172] Current objective function: 0.784017 = (wSIM)0.787272 - (wBE)3.45e-04 - (wJAC)2.91e-03 [+ 0.169467 mm]\n",
      "[NiftyReg F3D] [180] Current objective function: 0.784022 = (wSIM)0.787279 - (wBE)3.49e-04 - (wJAC)2.91e-03 [+ 0.0105917 mm]\n",
      "[NiftyReg F3D] [190] Current objective function: 0.784283 = (wSIM)0.787538 - (wBE)3.47e-04 - (wJAC)2.91e-03 [+ 0.0817212 mm]\n",
      "[NiftyReg F3D] [199] Current objective function: 0.784291 = (wSIM)0.787543 - (wBE)3.47e-04 - (wJAC)2.91e-03 [+ 0.0158335 mm]\n",
      "[NiftyReg F3D] [206] Current objective function: 0.78439 = (wSIM)0.787637 - (wBE)3.48e-04 - (wJAC)2.90e-03 [+ 0.0428296 mm]\n",
      "[NiftyReg F3D] [212] Current objective function: 0.784397 = (wSIM)0.78764 - (wBE)3.52e-04 - (wJAC)2.89e-03 [+ 0.0214148 mm]\n",
      "[NiftyReg F3D] [220] Current objective function: 0.78456 = (wSIM)0.787812 - (wBE)3.60e-04 - (wJAC)2.89e-03 [+ 0.0708829 mm]\n",
      "[NiftyReg F3D] [227] Current objective function: 0.784609 = (wSIM)0.787874 - (wBE)3.80e-04 - (wJAC)2.88e-03 [+ 0.0708829 mm]\n",
      "[NiftyReg F3D] [234] Current objective function: 0.784756 = (wSIM)0.788011 - (wBE)3.82e-04 - (wJAC)2.87e-03 [+ 0.0708829 mm]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NiftyReg F3D] [240] Current objective function: 0.784756 = (wSIM)0.788011 - (wBE)3.82e-04 - (wJAC)2.87e-03 [+ 0 mm]\n",
      "[NiftyReg F3D] Current registration level done\n",
      "[NiftyReg F3D] --------------------------------------------------\n",
      "[NiftyReg F3D] Registration Performed in 0 min 55 sec\n",
      "[NiftyReg F3D] Have a good day !\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/T2w/testDataBiasBet.nii.gz -flo /aida/lib/average_template_50.nii.gz -cpp /testData/T2w/testDataBiasBetMatrixBspline.nii -res /testData/T2w/testDataBiasBet_TemplateAllen.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/T2w/testDataBiasBet.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "Floating image name: /aida/lib/average_template_50.nii.gz\n",
      "\t228x160x264 voxels, 1 volumes\n",
      "\t0.05x0.05x0.05 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/T2w/testDataBiasBet_TemplateAllen.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/T2w/testDataBiasBet.nii.gz -flo /aida/lib/annoVolume+2000_rsfMRI.nii.gz -inter 0 -cpp /testData/T2w/testDataBiasBetMatrixBspline.nii -res /testData/T2w/testDataBiasBet_AnnorsfMRI.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/T2w/testDataBiasBet.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "Floating image name: /aida/lib/annoVolume+2000_rsfMRI.nii.gz\n",
      "\t228x160x264 voxels, 1 volumes\n",
      "\t0.05x0.05x0.05 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/T2w/testDataBiasBet_AnnorsfMRI.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/T2w/testDataBiasBet.nii.gz -flo /aida/lib/annotation_50CHANGEDanno.nii.gz -inter 0 -cpp /testData/T2w/testDataBiasBetMatrixBspline.nii -res /testData/T2w/testDataBiasBet_Anno.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/T2w/testDataBiasBet.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "Floating image name: /aida/lib/annotation_50CHANGEDanno.nii.gz\n",
      "\t228x160x264 voxels, 1 volumes\n",
      "\t0.05x0.05x0.05 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/T2w/testDataBiasBet_Anno.nii.gz\n",
      "T2 Registration  \u001b[0;30;42m COMPLETED \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /aida/bin/2.1_T2PreProcessing aidamri \\\n",
    "python registration_T2.py -i /testData/T2w/testDataBiasBet.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stroke_mask.nii.gz\r\n",
      "preprocess.log\r\n",
      "reg.log\r\n",
      "testData.5.1.nii.gz\r\n",
      "testDataBias.nii.gz\r\n",
      "testDataBiasBet.nii.gz\r\n",
      "testDataBiasBetMatrixAff.txt\r\n",
      "testDataBiasBetMatrixBspline.nii\r\n",
      "testDataBiasBetMatrixInv.txt\r\n",
      "testDataBiasBet_Anno.nii.gz\r\n",
      "testDataBiasBet_AnnorsfMRI.nii.gz\r\n",
      "testDataBiasBet_IncidenceData.nii.gz\r\n",
      "testDataBiasBet_IncidenceData_mask.nii.gz\r\n",
      "testDataBiasBet_Template.nii.gz\r\n",
      "testDataBiasBet_TemplateAff.nii.gz\r\n",
      "testDataBiasBet_TemplateAllen.nii.gz\r\n",
      "testDataBiasBet_mask.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/T2w aidamri \\\n",
    "ls -t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new files are:\n",
    "* testDataBiasBet_Anno.nii.gz (anatomical Atlas-registered segmentation mask)\n",
    "* testDataBiasBet_AnnorsfMRI.nii.gz (functional Atlas-registered segmentation mask?)\n",
    "* testDataBiasBet_IncidenceData.nii.gz (<span style=color:red;font-size:bold;>?</span>)\n",
    "* testDataBiasBet_IncidenceData_mask.nii.gz (<span style=color:red;font-size:bold;>?</span>)\n",
    "* testDataBiasBet_Template.nii.gz (<span style=color:red;font-size:bold;>?</span>)\n",
    "* testDataBiasBet_TemplateAff.nii.gz (<span style=color:red;font-size:bold;>?</span>)\n",
    "* testDataBiasBet_TemplateAllen.nii.gz (<span style=color:red;font-size:bold;>?</span>)\n",
    "* testDataBiasBetMatrixBspline (<span style=color:red;font-size:bold;>?</span>)\n",
    "* testDataBiasBetMatrixInv.txt (<span style=color:red;font-size:bold;>?</span>)\n",
    "* testDataBiasBetMatrixAff.txt (<span style=color:red;font-size:bold;>?</span>)\n",
    "* reg.log\n",
    "\n",
    "It is recommended to use ITK-Snap to check the registration results by superimposing the Atlas segmentation over the extracted brain, i.e. drag in the bias bet file into ITKSnap and load it as main image, then load in the anno file as segmentation (see image below).\n",
    "\n",
    "![T2w registered brain image](images/itk_testData_t2w_cropped.png)\n",
    "\n",
    "Finally, extract the incidence size, the parental incidence size and the affected regions. To safe some typing, the commands are put into the same docker exec directive by using `bash -c` command. Basically, we pass consecutive commands as a string to the `bash -c` command, separated by a semicolon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1' folder will be proccessed...\n",
      "'1' folder will be proccessed...\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /aida/bin/3.1_T2Processing aidamri \\\n",
    "bash -c \"\\\n",
    "python getIncidenceSize_par.py -i /testData/T2w ; \\\n",
    "python getIncidenceSize.py -i /testData/T2w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stroke_mask.nii.gz\r\n",
      "affectedRegions.nii.gz\r\n",
      "affectedRegions.txt\r\n",
      "affectedRegions_Parental.nii.gz\r\n",
      "affectedRegions_Parental.txt\r\n",
      "labelCount.mat\r\n",
      "labelCount_par.mat\r\n",
      "preprocess.log\r\n",
      "reg.log\r\n",
      "testData.5.1.nii.gz\r\n",
      "testDataBias.nii.gz\r\n",
      "testDataBiasBet.nii.gz\r\n",
      "testDataBiasBetMatrixAff.txt\r\n",
      "testDataBiasBetMatrixBspline.nii\r\n",
      "testDataBiasBetMatrixInv.txt\r\n",
      "testDataBiasBet_Anno.nii.gz\r\n",
      "testDataBiasBet_AnnorsfMRI.nii.gz\r\n",
      "testDataBiasBet_IncidenceData.nii.gz\r\n",
      "testDataBiasBet_IncidenceDataAnno_mask.nii.gz\r\n",
      "testDataBiasBet_IncidenceDataAnno_parmask.nii.gz\r\n",
      "testDataBiasBet_IncidenceData_mask.nii.gz\r\n",
      "testDataBiasBet_Template.nii.gz\r\n",
      "testDataBiasBet_TemplateAff.nii.gz\r\n",
      "testDataBiasBet_TemplateAllen.nii.gz\r\n",
      "testDataBiasBet_mask.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/T2w aidamri \\\n",
    "ls -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following files now should be present:\n",
    "* testDataBiasBet_IncidenceDataAnno_parmask.nii.gz\n",
    "* affectedRegions_Parental.nii.gz\n",
    "* affectedRegions_Parental.txt\n",
    "* labelCount_par.mat\n",
    "* testDataBiasBet_IncidenceDataAnno_mask.nii.gz\n",
    "* affectedRegions.nii.gz\n",
    "* affectedRegions.txt\n",
    "* labelCount.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTI<a class=\"anchor\" id=\"dti\"></a>\n",
    "Similar to T2w, it begins with pre-processing. The test data is located in the DTI folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testData.7.1.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/DTI aidamri \\\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing includes dimension reduction, bias correction, threshold application and brain extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTI Preprocessing  \u001b[5m...\u001b[0m (wait!)\r",
      "DTI Preprocessing  \u001b[0;30;42m COMPLETED \u001b[0m\r\n",
      "  0% |                                                                        |\r",
      "  5% |###                                                                     |\r",
      " 10% |#######                                                                 |\r",
      " 15% |##########                                                              |\r",
      " 20% |##############                                                          |\r",
      " 25% |##################                                                      |\r",
      " 30% |#####################                                                   |\r",
      " 35% |#########################                                               |\r",
      " 40% |############################                                            |\r",
      " 45% |################################                                        |\r",
      " 50% |####################################                                    |\r",
      " 55% |#######################################                                 |\r",
      " 60% |###########################################                             |\r",
      " 65% |##############################################                          |\r",
      " 70% |##################################################                      |\r",
      " 75% |######################################################                  |\r",
      " 80% |#########################################################               |\r",
      " 85% |#############################################################           |\r",
      " 90% |################################################################        |\r",
      " 95% |####################################################################    |\r",
      "100% |########################################################################|\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /aida/bin/2.2_DTIPreProcessing aidamri \\\n",
    "python preProcessing_DTI.py -i /testData/DTI/testData.7.1.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess.log\r\n",
      "testData.7.1.nii.gz\r\n",
      "testDataDN.nii.gz\r\n",
      "testDataDNSmooth.nii.gz\r\n",
      "testDataDNSmoothMico.nii.gz\r\n",
      "testDataDNSmoothMicoBet.nii.gz\r\n",
      "testDataDNSmoothMicoBet_mask.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/DTI aidamri \\\n",
    "ls -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside the pre-process log file there are the following files after execution:\n",
    "* testDataDN.nii.gz\n",
    "* testDataDNSmooth.nii.gz\n",
    "* testDataDNSmoothMico.nii.gz\n",
    "* testDataDNSmoothMicoBet.nii.gz\n",
    "* testDataDNSmoothMicoBet_mask.nii.gz\n",
    "\n",
    "Afterwards, proceed with registration using the brain extracted file. The reference stroke mask used is already located in the folder. It is possible to use another reference mask from other days or dataset by using the `-r [STROKE-MASK-FILENAME]` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NiftyReg ALADIN] Command line:\n",
      "\t reg_aladin -ref /testData/DTI/testDataDNSmoothMicoBet.nii.gz -flo /testData/T2w/testDataBiasBet.nii.gz -res /testData/DTI/testDataDNSmoothMicoBet_T2w.nii.gz -rigOnly -aff /testData/DTI/testDataDNSmoothMicoBettransMatrixAff.txt\n",
      "\n",
      "[reg_aladin_sym] Parameters\n",
      "[reg_aladin_sym] Reference image name: /testData/DTI/testDataDNSmoothMicoBet.nii.gz\n",
      "[reg_aladin_sym] \t128x128x20 voxels\n",
      "[reg_aladin_sym] \t0.140625x0.140625x0.4 mm\n",
      "[reg_aladin_sym] Floating image name: /testData/T2w/testDataBiasBet.nii.gz\n",
      "[reg_aladin_sym] \t256x256x48 voxels\n",
      "[reg_aladin_sym] \t0.0683594x0.0683594x0.3 mm\n",
      "[reg_aladin_sym] Maximum iteration number: 5 (10 during the first level)\n",
      "[reg_aladin_sym] Percentage of blocks: 50 %\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Current level 1 / 3\n",
      "[reg_aladin_sym] reference image size: \t32x32x20 voxels\t0.5625x0.5625x0.4 mm\n",
      "[reg_aladin_sym] floating image size: \t64x64x48 voxels\t0.273438x0.273438x0.3 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [8 8 5]\n",
      "[reg_aladin_sym] Backward Block number = [16 16 12]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "1\t0\t0\t-0.25\n",
      "0\t1\t0\t-0.25\n",
      "0\t0\t1\t3.2\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "1\t0\t0\t0.25\n",
      "0\t1\t0\t0.25\n",
      "0\t0\t1\t-3.2\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "0.998172\t0.0275713\t0.0537913\t-0.118758\n",
      "-0.0297107\t0.998782\t0.0393854\t0.111694\n",
      "-0.0526399\t-0.0409116\t0.997775\t4.09616\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.998172\t-0.0297107\t-0.0526399\t0.337481\n",
      "0.0275713\t0.998782\t-0.0409116\t0.0592969\n",
      "0.0537913\t0.0393854\t0.997775\t-4.08506\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[reg_aladin_sym] Current level 2 / 3\n",
      "[reg_aladin_sym] reference image size: \t64x64x20 voxels\t0.28125x0.28125x0.4 mm\n",
      "[reg_aladin_sym] floating image size: \t128x128x48 voxels\t0.136719x0.136719x0.3 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [16 16 5]\n",
      "[reg_aladin_sym] Backward Block number = [32 32 12]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "0.998172\t0.0275713\t0.0537913\t-0.118758\n",
      "-0.0297107\t0.998782\t0.0393854\t0.111694\n",
      "-0.0526399\t-0.0409116\t0.997775\t4.09616\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "0.998172\t-0.0297107\t-0.0526399\t0.337481\n",
      "0.0275713\t0.998782\t-0.0409116\t0.0592969\n",
      "0.0537913\t0.0393854\t0.997775\t-4.08506\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "0.999048\t0.0108569\t0.0422583\t0.0920481\n",
      "-0.013317\t0.998206\t0.0583748\t-0.194553\n",
      "-0.0415487\t-0.058882\t0.9974\t4.13923\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.999048\t-0.013317\t-0.0415487\t0.0774287\n",
      "0.0108569\t0.998206\t-0.058882\t0.436931\n",
      "0.0422583\t0.0583748\t0.9974\t-4.12101\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[reg_aladin_sym] Current level 3 / 3\n",
      "[reg_aladin_sym] reference image size: \t128x128x20 voxels\t0.140625x0.140625x0.4 mm\n",
      "[reg_aladin_sym] floating image size: \t256x256x48 voxels\t0.0683594x0.0683594x0.3 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [32 32 5]\n",
      "[reg_aladin_sym] Backward Block number = [64 64 12]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "0.999048\t0.0108569\t0.0422583\t0.0920481\n",
      "-0.013317\t0.998206\t0.0583748\t-0.194553\n",
      "-0.0415487\t-0.058882\t0.9974\t4.13923\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "0.999048\t-0.013317\t-0.0415487\t0.0774287\n",
      "0.0108569\t0.998206\t-0.058882\t0.436931\n",
      "0.0422583\t0.0583748\t0.9974\t-4.12101\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "0.999877\t0.00773625\t0.013682\t0.195948\n",
      "-0.00810399\t0.999602\t0.0270294\t-0.227389\n",
      "-0.0134674\t-0.0271369\t0.999541\t3.59453\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.999876\t-0.00810399\t-0.0134674\t-0.149358\n",
      "0.00773625\t0.999602\t-0.0271369\t0.323327\n",
      "0.013682\t0.0270294\t0.999541\t-3.58941\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "Registration Performed in 0 min 4 sec\n",
      "Have a good day !\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/T2w/testDataBiasBet_Anno.nii.gz -flo /aida/lib/ARA_annotationR+2000.nii.gz -trans /testData/T2w/testDataBiasBetMatrixBspline.nii -inter 0 -res /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/T2w/testDataBiasBet_Anno.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "Floating image name: /aida/lib/ARA_annotationR+2000.nii.gz\n",
      "\t228x160x264 voxels, 1 volumes\n",
      "\t0.05x0.05x0.05 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/DTI/testDataDNSmoothMicoBet.nii.gz -flo /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit.nii.gz -trans /testData/DTI/testDataDNSmoothMicoBettransMatrixAff.txt -inter 0 -res /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/DTI/testDataDNSmoothMicoBet.nii.gz\n",
      "\t128x128x20 voxels, 1 volumes\n",
      "\t0.140625x0.140625x0.4 mm\n",
      "Floating image name: /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/T2w/testDataBiasBet_Anno.nii.gz -flo /aida/lib/annoVolume+2000_rsfMRI.nii.gz -trans /testData/T2w/testDataBiasBetMatrixBspline.nii -inter 0 -res /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit_rsfMRI.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/T2w/testDataBiasBet_Anno.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "Floating image name: /aida/lib/annoVolume+2000_rsfMRI.nii.gz\n",
      "\t228x160x264 voxels, 1 volumes\n",
      "\t0.05x0.05x0.05 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit_rsfMRI.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/DTI/testDataDNSmoothMicoBet.nii.gz -flo /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit_rsfMRI.nii.gz -trans /testData/DTI/testDataDNSmoothMicoBettransMatrixAff.txt -inter 0 -res /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit_rsfMRI.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/DTI/testDataDNSmoothMicoBet.nii.gz\n",
      "\t128x128x20 voxels, 1 volumes\n",
      "\t0.140625x0.140625x0.4 mm\n",
      "Floating image name: /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit_rsfMRI.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/DTI/testDataDNSmoothMicoBet_AnnoSplit_rsfMRI.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/T2w/testDataBiasBet_Anno.nii.gz -flo /aida/lib/annoVolume.nii.gz -trans /testData/T2w/testDataBiasBetMatrixBspline.nii -inter 0 -res /testData/DTI/testDataDNSmoothMicoBet_Anno_rsfMRI.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/T2w/testDataBiasBet_Anno.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "Floating image name: /aida/lib/annoVolume.nii.gz\n",
      "\t228x160x264 voxels, 1 volumes\n",
      "\t0.05x0.05x0.05 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/DTI/testDataDNSmoothMicoBet_Anno_rsfMRI.nii.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/DTI/testDataDNSmoothMicoBet.nii.gz -flo /testData/DTI/testDataDNSmoothMicoBet_Anno_rsfMRI.nii.gz -trans /testData/DTI/testDataDNSmoothMicoBettransMatrixAff.txt -inter 0 -res /testData/DTI/testDataDNSmoothMicoBet_Anno_rsfMRI.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/DTI/testDataDNSmoothMicoBet.nii.gz\n",
      "\t128x128x20 voxels, 1 volumes\n",
      "\t0.140625x0.140625x0.4 mm\n",
      "Floating image name: /testData/DTI/testDataDNSmoothMicoBet_Anno_rsfMRI.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/DTI/testDataDNSmoothMicoBet_Anno_rsfMRI.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/DTI/testDataDNSmoothMicoBet.nii.gz -flo /testData/T2w/testDataBiasBet_TemplateAllen.nii.gz -cpp /testData/DTI/testDataDNSmoothMicoBettransMatrixAff.txt -res /testData/DTI/testDataDNSmoothMicoBet_Template.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/DTI/testDataDNSmoothMicoBet.nii.gz\n",
      "\t128x128x20 voxels, 1 volumes\n",
      "\t0.140625x0.140625x0.4 mm\n",
      "Floating image name: /testData/T2w/testDataBiasBet_TemplateAllen.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/DTI/testDataDNSmoothMicoBet_Template.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/DTI/testDataDNSmoothMicoBet.nii.gz -flo /testData/T2w/Stroke_mask.nii.gz -inter 0 -cpp /testData/DTI/testDataDNSmoothMicoBettransMatrixAff.txt -res /testData/DTI/testDataDNSmoothMicoBetStroke_mask.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/DTI/testDataDNSmoothMicoBet.nii.gz\n",
      "\t128x128x20 voxels, 1 volumes\n",
      "\t0.140625x0.140625x0.4 mm\n",
      "Floating image name: /testData/T2w/Stroke_mask.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/DTI/testDataDNSmoothMicoBetStroke_mask.nii.gz\n",
      "DTI Registration  \u001b[0;30;42m COMPLETED \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /aida/bin/2.2_DTIPreProcessing aidamri \\\n",
    "python registration_DTI.py -i /testData/DTI/testDataDNSmoothMicoBet.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the registration can be seen below. The rsfMRI segmentation was used in this case.\n",
    "![DTI registered brain image](images/itk_testData_DTI_cropped.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testDataDNSmoothMicoBetAllen_scaled.nii\r\n",
      "testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.nii\r\n",
      "testDataDNSmoothMicoBetAnno_scaled.nii\r\n",
      "testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.txt\r\n",
      "testDataDNSmoothMicoBetAnno_scaled.txt\r\n",
      "testDataDNSmoothMicoBetMask_scaled.nii\r\n",
      "testDataDNSmoothMicoBetrsfMRI_Mask_scaled.nii\r\n",
      "testDataDNSmoothMicoBetrsfMRI_Mask_scaled.txt\r\n",
      "testDataDNSmoothMicoBetStrokeMask_scaled.nii\r\n",
      "testDataDNSmoothMicoBetStrokeMask_scaled.txt\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/DTI/DSI_studio aidamri ls -t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following files and folders were added:\n",
    "* registration.log\n",
    "* DSI_studio\n",
    "    - testDataDNSmoothMicoBetAllen_scaled.nii\n",
    "    - testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.nii\n",
    "    - testDataDNSmoothMicoBetAnno_scaled.nii\n",
    "    - testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.txt\n",
    "    - testDataDNSmoothMicoBetAnno_scaled.txt\n",
    "    - testDataDNSmoothMicoBetMask_scaled.nii\n",
    "    - testDataDNSmoothMicoBetrsfMRI_Mask_scaled.txt\n",
    "    - testDataDNSmoothMicoBetrsfMRI_Mask_scaled.nii\n",
    "    - testDataDNSmoothMicoBetStrokeMask_scaled.txt\n",
    "    - testDataDNSmoothMicoBetStrokeMask_scaled.nii\n",
    "* testDataDNSmoothMicoBetAnno_rsfMRI_mask.nii.gz\n",
    "* testDataDNSmoothMicoBetAnno_mask.nii.gz\n",
    "* testDataDNSmoothMicoBetStroke_mask.nii.gz\n",
    "* testDataDNSmoothMicoBet_Template.nii.gz\n",
    "* testDataDNSmoothMicoBet_Anno_rsfMRI.nii.gz\n",
    "* testDataDNSmoothMicoBet_AnnoSplit_rsfMRI.nii.gz\n",
    "* testDataDNSmoothMicoBet_AnnoSplit.nii.gz\n",
    "* testDataDNSmoothMicoBettransMatrixAff.txt\n",
    "* testDataDNSmoothMicoBet_T2w.nii.gz\n",
    "\n",
    "Connectivity can be calculated by using the DSI studio software, invoked via the dsi_main Python protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=testData_mcf.nii.gz\n",
      "action=src\n",
      "reading testData_mcf.nii.gz\n",
      "b_table=/aida/lib/DTI_Jones30.txt\n",
      "b-table /aida/lib/DTI_Jones30.txt loaded\n",
      "output=/testData/DTI/src/testData_mcf.nii.src.gz\n",
      "output src to /testData/DTI/src/testData_mcf.nii.src.gz\n",
      "sort_b_table=0\n",
      "up_sampling=0\n",
      "save testData_mcf.nii.src.gz\n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/src/testData_mcf.nii.src.gz\n",
      "action=rec\n",
      "loading source...\n",
      "SRC file loaded\n",
      "src loaded\n",
      "mask=/testData/DTI/DSI_studio/testDataDNSmoothMicoBetMask_scaled.nii\n",
      "param0=16\n",
      "method=1\n",
      "template=1\n",
      "odf_order=8\n",
      "odf_resolving=0\n",
      "record_odf=0\n",
      "dti_no_high_b=0\n",
      "check_btable=0\n",
      "other_output=fa,ad,rd,md,nqa,iso,rdi,nrdi\n",
      "num_fiber=5\n",
      "r2_weighted=0\n",
      "thread_count=8\n",
      "half_sphere=1\n",
      "scheme_balance=1\n",
      "start reconstruction...\n",
      "DTI\n",
      "saving testData_mcf.nii.src.gz.dti.fib.gz\n",
      "reconstruction finished.\n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=exp\n",
      "export=fa\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "fa.nii.gz saved \n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=exp\n",
      "export=md\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "md loaded\n",
      "md.nii.gz saved \n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=exp\n",
      "export=ad\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "ad loaded\n",
      "ad.nii.gz saved \n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=exp\n",
      "export=rd\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "rd loaded\n",
      "rd.nii.gz saved \n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=trk\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "otsu_threshold=0.6\n",
      "fa_threshold=.02\n",
      "dt_threshold=0.2\n",
      "turning_angle=55\n",
      "step_size=.5\n",
      "smoothing=.1\n",
      "min_length=.5\n",
      "max_length=12.0\n",
      "method=0\n",
      "initial_dir=0\n",
      "check_ending=0\n",
      "tip_iteration=0\n",
      "fiber_count=1000000\n",
      "seed_count=0\n",
      "thread_count=8\n",
      "start tracking.\n",
      "thread_count=8\n",
      "finished tracking.\n",
      "1000000 tracts are generated using 1033230 seeds.\n",
      "0 tracts are removed by pruning.\n",
      "The final analysis results in 1000000 tracts.\n",
      "output=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "saving testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "Warning: --interpolation is not used. Please check command line syntax.\n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=ana\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "tract=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "loading testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "A total of 1000000 tracks loaded\n",
      "a total of 1 tract file(s) loaded\n",
      "connectivity=/testData/DTI/DSI_studio/testDataDNSmoothMicoBetStrokeMask_scaled.nii\n",
      "connectivity_type=pass,end\n",
      "connectivity_value=qa\n",
      "loading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetStrokeMask_scaled.nii\n",
      "reading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetStrokeMask_scaled.nii as a NIFTI regioin file\n",
      "DWI dimension=(128,128,20)\n",
      "NIFTI dimension=(128,128,20)\n",
      "nifti loaded as multiple ROI file\n",
      "looking for region label file\n",
      "label file loaded:/testData/DTI/DSI_studio/testDataDNSmoothMicoBetStrokeMask_scaled.txt\n",
      "a total of 45 regions are loaded.\n",
      "connectivity_threshold=0.001\n",
      "count tracks by passing\n",
      "calculate matrix using qa\n",
      "connectivity calculation error:Cannot quantify matrix value using qa\n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=ana\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "tract=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "loading testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "A total of 1000000 tracks loaded\n",
      "a total of 1 tract file(s) loaded\n",
      "connectivity=/testData/DTI/DSI_studio/testDataDNSmoothMicoBetStrokeMask_scaled.nii\n",
      "connectivity_type=pass,end\n",
      "connectivity_value=count\n",
      "loading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetStrokeMask_scaled.nii\n",
      "reading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetStrokeMask_scaled.nii as a NIFTI regioin file\n",
      "DWI dimension=(128,128,20)\n",
      "NIFTI dimension=(128,128,20)\n",
      "nifti loaded as multiple ROI file\n",
      "looking for region label file\n",
      "label file loaded:/testData/DTI/DSI_studio/testDataDNSmoothMicoBetStrokeMask_scaled.txt\n",
      "a total of 45 regions are loaded.\n",
      "connectivity_threshold=0.001\n",
      "count tracks by passing\n",
      "calculate matrix using count\n",
      "export connectivity matrix to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.pass.connectivity.mat\n",
      "export connectogram to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.pass.connectogram.txt\n",
      "export network measures to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.pass.network_measures.txt\n",
      "count tracks by ending\n",
      "calculate matrix using count\n",
      "export connectivity matrix to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.end.connectivity.mat\n",
      "export connectogram to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.end.connectogram.txt\n",
      "export network measures to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.end.network_measures.txt\n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=ana\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "tract=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "loading testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "A total of 1000000 tracks loaded\n",
      "a total of 1 tract file(s) loaded\n",
      "connectivity=/testData/DTI/DSI_studio/testDataDNSmoothMicoBetrsfMRI_Mask_scaled.nii\n",
      "connectivity_type=pass,end\n",
      "connectivity_value=qa\n",
      "loading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetrsfMRI_Mask_scaled.nii\n",
      "reading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetrsfMRI_Mask_scaled.nii as a NIFTI regioin file\n",
      "DWI dimension=(128,128,20)\n",
      "NIFTI dimension=(128,128,20)\n",
      "nifti loaded as multiple ROI file\n",
      "looking for region label file\n",
      "label file loaded:/testData/DTI/DSI_studio/testDataDNSmoothMicoBetrsfMRI_Mask_scaled.txt\n",
      "a total of 12 regions are loaded.\n",
      "connectivity_threshold=0.001\n",
      "count tracks by passing\n",
      "calculate matrix using qa\n",
      "connectivity calculation error:Cannot quantify matrix value using qa\n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=ana\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "tract=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "loading testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "A total of 1000000 tracks loaded\n",
      "a total of 1 tract file(s) loaded\n",
      "connectivity=/testData/DTI/DSI_studio/testDataDNSmoothMicoBetrsfMRI_Mask_scaled.nii\n",
      "connectivity_type=pass,end\n",
      "connectivity_value=count\n",
      "loading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetrsfMRI_Mask_scaled.nii\n",
      "reading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetrsfMRI_Mask_scaled.nii as a NIFTI regioin file\n",
      "DWI dimension=(128,128,20)\n",
      "NIFTI dimension=(128,128,20)\n",
      "nifti loaded as multiple ROI file\n",
      "looking for region label file\n",
      "label file loaded:/testData/DTI/DSI_studio/testDataDNSmoothMicoBetrsfMRI_Mask_scaled.txt\n",
      "a total of 12 regions are loaded.\n",
      "connectivity_threshold=0.001\n",
      "count tracks by passing\n",
      "calculate matrix using count\n",
      "export connectivity matrix to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.pass.connectivity.mat\n",
      "export connectogram to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.pass.connectogram.txt\n",
      "export network measures to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.pass.network_measures.txt\n",
      "count tracks by ending\n",
      "calculate matrix using count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export connectivity matrix to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.end.connectivity.mat\n",
      "export connectogram to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.end.connectogram.txt\n",
      "export network measures to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.end.network_measures.txt\n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=ana\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "tract=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "loading testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "A total of 1000000 tracks loaded\n",
      "a total of 1 tract file(s) loaded\n",
      "connectivity=/testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_scaled.nii\n",
      "connectivity_type=pass,end\n",
      "connectivity_value=qa\n",
      "loading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_scaled.nii\n",
      "reading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_scaled.nii as a NIFTI regioin file\n",
      "DWI dimension=(128,128,20)\n",
      "NIFTI dimension=(128,128,20)\n",
      "nifti loaded as multiple ROI file\n",
      "looking for region label file\n",
      "label file loaded:/testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_scaled.txt\n",
      "a total of 941 regions are loaded.\n",
      "connectivity_threshold=0.001\n",
      "count tracks by passing\n",
      "calculate matrix using qa\n",
      "connectivity calculation error:Cannot quantify matrix value using qa\n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=ana\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "tract=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "loading testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "A total of 1000000 tracks loaded\n",
      "a total of 1 tract file(s) loaded\n",
      "connectivity=/testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_scaled.nii\n",
      "connectivity_type=pass,end\n",
      "connectivity_value=count\n",
      "loading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_scaled.nii\n",
      "reading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_scaled.nii as a NIFTI regioin file\n",
      "DWI dimension=(128,128,20)\n",
      "NIFTI dimension=(128,128,20)\n",
      "nifti loaded as multiple ROI file\n",
      "looking for region label file\n",
      "label file loaded:/testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_scaled.txt\n",
      "a total of 941 regions are loaded.\n",
      "connectivity_threshold=0.001\n",
      "count tracks by passing\n",
      "calculate matrix using count\n",
      "export connectivity matrix to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.pass.connectivity.mat\n",
      "export connectogram to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.pass.connectogram.txt\n",
      "export network measures to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.pass.network_measures.txt\n",
      "count tracks by ending\n",
      "calculate matrix using count\n",
      "export connectivity matrix to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.end.connectivity.mat\n",
      "export connectogram to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.end.connectogram.txt\n",
      "export network measures to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.end.network_measures.txt\n",
      "fslsplit /testData/DTI/fslScaleTemp.nii.gz testData -z\n",
      "For all slices ... \n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0000.nii.gz -out /testData/DTI/mcf_Folder/testData0000.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0001.nii.gz -out /testData/DTI/mcf_Folder/testData0001.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0002.nii.gz -out /testData/DTI/mcf_Folder/testData0002.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0003.nii.gz -out /testData/DTI/mcf_Folder/testData0003.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0004.nii.gz -out /testData/DTI/mcf_Folder/testData0004.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0005.nii.gz -out /testData/DTI/mcf_Folder/testData0005.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0006.nii.gz -out /testData/DTI/mcf_Folder/testData0006.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0007.nii.gz -out /testData/DTI/mcf_Folder/testData0007.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0008.nii.gz -out /testData/DTI/mcf_Folder/testData0008.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0009.nii.gz -out /testData/DTI/mcf_Folder/testData0009.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0010.nii.gz -out /testData/DTI/mcf_Folder/testData0010.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0011.nii.gz -out /testData/DTI/mcf_Folder/testData0011.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0012.nii.gz -out /testData/DTI/mcf_Folder/testData0012.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0013.nii.gz -out /testData/DTI/mcf_Folder/testData0013.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0014.nii.gz -out /testData/DTI/mcf_Folder/testData0014.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0015.nii.gz -out /testData/DTI/mcf_Folder/testData0015.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0016.nii.gz -out /testData/DTI/mcf_Folder/testData0016.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0017.nii.gz -out /testData/DTI/mcf_Folder/testData0017.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0018.nii.gz -out /testData/DTI/mcf_Folder/testData0018.nii.gz -plots\n",
      "mcflirt -in /aida/bin/3.2_DTIConnectivity/testData0019.nii.gz -out /testData/DTI/mcf_Folder/testData0019.nii.gz -plots\n",
      "fslmerge -z /testData/DTI/testData_mcf.nii.gz /testData/DTI/mcf_Folder/testData0000.nii.gz /testData/DTI/mcf_Folder/testData0001.nii.gz /testData/DTI/mcf_Folder/testData0002.nii.gz /testData/DTI/mcf_Folder/testData0003.nii.gz /testData/DTI/mcf_Folder/testData0004.nii.gz /testData/DTI/mcf_Folder/testData0005.nii.gz /testData/DTI/mcf_Folder/testData0006.nii.gz /testData/DTI/mcf_Folder/testData0007.nii.gz /testData/DTI/mcf_Folder/testData0008.nii.gz /testData/DTI/mcf_Folder/testData0009.nii.gz /testData/DTI/mcf_Folder/testData0010.nii.gz /testData/DTI/mcf_Folder/testData0011.nii.gz /testData/DTI/mcf_Folder/testData0012.nii.gz /testData/DTI/mcf_Folder/testData0013.nii.gz /testData/DTI/mcf_Folder/testData0014.nii.gz /testData/DTI/mcf_Folder/testData0015.nii.gz /testData/DTI/mcf_Folder/testData0016.nii.gz /testData/DTI/mcf_Folder/testData0017.nii.gz /testData/DTI/mcf_Folder/testData0018.nii.gz /testData/DTI/mcf_Folder/testData0019.nii.gz\n",
      "Create directory \"/testData/DTI/src\"\n",
      "Create directory \"/testData/DTI/fib_map\"\n",
      "Generate src-File /aida/bin/dsi_studio_ubuntu_1804/dsi-studio/dsi_studio --action=src --source=testData_mcf.nii.gz --output=/testData/DTI/src/testData_mcf.nii.src.gz --b_table=/aida/lib/DTI_Jones30.txt:\n",
      "Generate fib-File /aida/bin/dsi_studio_ubuntu_1804/dsi-studio/dsi_studio --action=rec --source=/testData/DTI/src/testData_mcf.nii.src.gz --mask=/testData/DTI/DSI_studio/testDataDNSmoothMicoBetMask_scaled.nii --method=1 --param0=16 --check_btable=0 --half_sphere=1:\n",
      "Move file \"/testData/DTI/src/testData_mcf.nii.src.gz.dti.fib.gz\" to directory \"/testData/DTI/fib_map\"\n",
      "Generate two maps /aida/bin/dsi_studio_ubuntu_1804/dsi-studio/dsi_studio --action=exp --source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz --export=fa:\n",
      "Generate two maps /aida/bin/dsi_studio_ubuntu_1804/dsi-studio/dsi_studio --action=exp --source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz --export=md:\n",
      "Generate two maps /aida/bin/dsi_studio_ubuntu_1804/dsi-studio/dsi_studio --action=exp --source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz --export=ad:\n",
      "Generate two maps /aida/bin/dsi_studio_ubuntu_1804/dsi-studio/dsi_studio --action=exp --source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz --export=rd:\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.fa.nii.gz\" to directory \"/testData/DTI/DSI_studio\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.md.nii.gz\" to directory \"/testData/DTI/DSI_studio\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.ad.nii.gz\" to directory \"/testData/DTI/DSI_studio\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.rd.nii.gz\" to directory \"/testData/DTI/DSI_studio\"\n",
      "Track neuronal pathes /aida/bin/dsi_studio_ubuntu_1804/dsi-studio/dsi_studio --action=trk --source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz --output=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz --fiber_count=1000000 --interpolation=0 --step_size=.5 --turning_angle=55 --check_ending=0 --fa_threshold=.02 --smoothing=.1 --min_length=.5 --max_length=12.0:\n",
      "Create directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.end.connectogram.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.end.network_measures.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.pass.connectogram.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.pass.network_measures.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.end.connectivity.mat\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetStrokeMask_scaled.count.pass.connectivity.mat\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.end.connectogram.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.end.network_measures.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.pass.connectogram.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.pass.network_measures.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.end.connectivity.mat\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetrsfMRI_Mask_scaled.count.pass.connectivity.mat\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.end.connectogram.txt\" to directory \"/testData/DTI/connectivity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.end.network_measures.txt\" to directory \"/testData/DTI/connectivity\"DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=ana\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "tract=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "loading testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "A total of 1000000 tracks loaded\n",
      "a total of 1 tract file(s) loaded\n",
      "connectivity=/testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.nii\n",
      "connectivity_type=pass,end\n",
      "connectivity_value=qa\n",
      "loading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.nii\n",
      "reading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.nii as a NIFTI regioin file\n",
      "DWI dimension=(128,128,20)\n",
      "NIFTI dimension=(128,128,20)\n",
      "nifti loaded as multiple ROI file\n",
      "looking for region label file\n",
      "label file loaded:/testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.txt\n",
      "a total of 94 regions are loaded.\n",
      "connectivity_threshold=0.001\n",
      "count tracks by passing\n",
      "calculate matrix using qa\n",
      "connectivity calculation error:Cannot quantify matrix value using qa\n",
      "DSI Studio \"Chen\" Apr 14 2022\n",
      "source=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz\n",
      "action=ana\n",
      "loading /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz...\n",
      "FIB file loaded\n",
      "tract=/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "loading testData_mcf.nii.src.gz.dti.fib.gz.trk.gz\n",
      "A total of 1000000 tracks loaded\n",
      "a total of 1 tract file(s) loaded\n",
      "connectivity=/testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.nii\n",
      "connectivity_type=pass,end\n",
      "connectivity_value=count\n",
      "loading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.nii\n",
      "reading /testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.nii as a NIFTI regioin file\n",
      "DWI dimension=(128,128,20)\n",
      "NIFTI dimension=(128,128,20)\n",
      "nifti loaded as multiple ROI file\n",
      "looking for region label file\n",
      "label file loaded:/testData/DTI/DSI_studio/testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.txt\n",
      "a total of 94 regions are loaded.\n",
      "connectivity_threshold=0.001\n",
      "count tracks by passing\n",
      "calculate matrix using count\n",
      "export connectivity matrix to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.pass.connectivity.mat\n",
      "export connectogram to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.pass.connectogram.txt\n",
      "export network measures to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.pass.network_measures.txt\n",
      "count tracks by ending\n",
      "calculate matrix using count\n",
      "export connectivity matrix to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.end.connectivity.mat\n",
      "export connectogram to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.end.connectogram.txt\n",
      "export network measures to /testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.end.network_measures.txt\n",
      "\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.pass.connectogram.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.pass.network_measures.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.end.connectivity.mat\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_scaled.count.pass.connectivity.mat\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.end.connectogram.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.end.network_measures.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.pass.connectogram.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.pass.network_measures.txt\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.end.connectivity.mat\" to directory \"/testData/DTI/connectivity\"\n",
      "Move file \"/testData/DTI/fib_map/testData_mcf.nii.src.gz.dti.fib.gz.trk.gz.testDataDNSmoothMicoBetAnno_rsfMRISplit_scaled.count.pass.connectivity.mat\" to directory \"/testData/DTI/connectivity\"\n",
      "DTI Connectivity  \u001b[0;30;42m COMPLETED \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /aida/bin/3.2_DTIConnectivity aidamri \\\n",
    "python dsi_main.py -i /testData/DTI/testData.7.1.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process proceeded a folder structure, packed with processed data, including fiber mapping and connectivity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connectivity\r\n",
      "fib_map\r\n",
      "DSI_studio\r\n",
      "src\r\n",
      "testData_mcf.nii.gz\r\n",
      "mcf_Folder\r\n",
      "registration.log\r\n",
      "testDataDNSmoothMicoBetAnno_rsfMRI_mask.nii.gz\r\n",
      "testDataDNSmoothMicoBetAnno_mask.nii.gz\r\n",
      "testDataDNSmoothMicoBetStroke_mask.nii.gz\r\n",
      "testDataDNSmoothMicoBet_Template.nii.gz\r\n",
      "testDataDNSmoothMicoBet_Anno_rsfMRI.nii.gz\r\n",
      "testDataDNSmoothMicoBet_AnnoSplit_rsfMRI.nii.gz\r\n",
      "testDataDNSmoothMicoBet_AnnoSplit.nii.gz\r\n",
      "testDataDNSmoothMicoBet_T2w.nii.gz\r\n",
      "testDataDNSmoothMicoBettransMatrixAff.txt\r\n",
      "preprocess.log\r\n",
      "testDataDNSmoothMicoBet.nii.gz\r\n",
      "testDataDNSmoothMicoBet_mask.nii.gz\r\n",
      "testDataDNSmoothMico.nii.gz\r\n",
      "testDataDNSmooth.nii.gz\r\n",
      "testDataDN.nii.gz\r\n",
      "testData.7.1.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/DTI aidamri ls -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the new folder list. Use `ls` on the different folders to see the contents. Alternatively, directly click through them in your data folder.\n",
    "* connectivity\n",
    "* fib_map\n",
    "* DSI_studio\n",
    "* src\n",
    "* mcf_Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fMRI<a class=\"anchor\" id=\"fmri\"></a>\n",
    "Pre-processing again is started like the other method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsfMRI Preprocessing  \u001b[5m...\u001b[0m (wait!)\r",
      "rsfMRI Preprocessing  \u001b[0;30;42m COMPLETED \u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /aida/bin/2.3_fMRIPreProcessing aidamri \\\n",
    "python preProcessing_fMRI.py -i /testData/fMRI/testData.6.1.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data after processing will be similar to previous pre-processing steps. Check below to see the file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess.log\r\n",
      "testDataSmoothBet.nii.gz\r\n",
      "testDataSmoothBet_mask.nii.gz\r\n",
      "testDataSmooth.nii.gz\r\n",
      "testDataDN.nii.gz\r\n",
      "testData.6.1.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/fMRI aidamri ls -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be, again, the registration used on the brain extraction file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NiftyReg ALADIN] Command line:\n",
      "\t reg_aladin -ref /testData/fMRI/testDataSmoothBet.nii.gz -flo /testData/T2w/testDataBiasBet.nii.gz -res /testData/fMRI/testDataSmoothBet_T2w.nii.gz -aff /testData/fMRI/testDataSmoothBettransMatrixAff.txt\n",
      "\n",
      "[reg_aladin_sym] Parameters\n",
      "[reg_aladin_sym] Reference image name: /testData/fMRI/testDataSmoothBet.nii.gz\n",
      "[reg_aladin_sym] \t128x128x20 voxels\n",
      "[reg_aladin_sym] \t0.140625x0.140625x0.4 mm\n",
      "[reg_aladin_sym] Floating image name: /testData/T2w/testDataBiasBet.nii.gz\n",
      "[reg_aladin_sym] \t256x256x48 voxels\n",
      "[reg_aladin_sym] \t0.0683594x0.0683594x0.3 mm\n",
      "[reg_aladin_sym] Maximum iteration number: 5 (10 during the first level)\n",
      "[reg_aladin_sym] Percentage of blocks: 50 %\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Current level 1 / 3\n",
      "[reg_aladin_sym] reference image size: \t32x32x20 voxels\t0.5625x0.5625x0.4 mm\n",
      "[reg_aladin_sym] floating image size: \t64x64x48 voxels\t0.273438x0.273438x0.3 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [8 8 5]\n",
      "[reg_aladin_sym] Backward Block number = [16 16 12]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "1\t0\t0\t-0.25\n",
      "0\t1\t0\t-0.25\n",
      "0\t0\t1\t3.2\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "1\t0\t0\t0.25\n",
      "0\t1\t0\t0.25\n",
      "0\t0\t1\t-3.2\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "1.0124\t0.116708\t0.0210427\t-0.910679\n",
      "-0.0635667\t1.11402\t0.0561056\t-0.612335\n",
      "0.0339436\t-0.391846\t1.37229\t4.14866\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.981422\t-0.106577\t-0.0106917\t0.872856\n",
      "0.0564118\t0.878797\t-0.0367942\t0.742138\n",
      "-0.00816754\t0.253569\t0.718465\t-2.83284\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[reg_aladin_sym] Current level 2 / 3\n",
      "[reg_aladin_sym] reference image size: \t64x64x20 voxels\t0.28125x0.28125x0.4 mm\n",
      "[reg_aladin_sym] floating image size: \t128x128x48 voxels\t0.136719x0.136719x0.3 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [16 16 5]\n",
      "[reg_aladin_sym] Backward Block number = [32 32 12]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "1.0124\t0.116708\t0.0210427\t-0.910679\n",
      "-0.0635667\t1.11402\t0.0561056\t-0.612335\n",
      "0.0339436\t-0.391846\t1.37229\t4.14866\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "0.981422\t-0.106577\t-0.0106917\t0.872856\n",
      "0.0564118\t0.878797\t-0.0367942\t0.742138\n",
      "-0.00816754\t0.253569\t0.718465\t-2.83284\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "1.05737\t-0.00110787\t-0.031221\t-0.0646289\n",
      "-0.0266323\t1.196\t0.0469169\t-1.48408\n",
      "0.0776352\t-0.399896\t1.39058\t4.1041\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.944403\t0.00787561\t0.0209379\t-0.0132072\n",
      "0.0228404\t0.826982\t-0.0273889\t1.34119\n",
      "-0.0461572\t0.23738\t0.710081\t-2.56493\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[reg_aladin_sym] Current level 3 / 3\n",
      "[reg_aladin_sym] reference image size: \t128x128x20 voxels\t0.140625x0.140625x0.4 mm\n",
      "[reg_aladin_sym] floating image size: \t256x256x48 voxels\t0.0683594x0.0683594x0.3 mm\n",
      "[reg_aladin_sym] Block size = [4 4 4]\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Forward Block number = [32 32 5]\n",
      "[reg_aladin_sym] Backward Block number = [64 64 12]\n",
      "[reg_aladin_sym] Initial forward transformation matrix::\n",
      "1.05737\t-0.00110787\t-0.031221\t-0.0646289\n",
      "-0.0266323\t1.196\t0.0469169\t-1.48408\n",
      "0.0776352\t-0.399896\t1.39058\t4.1041\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Initial backward transformation matrix::\n",
      "0.944403\t0.00787561\t0.0209379\t-0.0132072\n",
      "0.0228404\t0.826982\t-0.0273889\t1.34119\n",
      "-0.0461572\t0.23738\t0.710081\t-2.56493\n",
      "0\t0\t0\t1\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "[reg_aladin_sym] Final forward transformation matrix::\n",
      "1.07167\t-0.00307272\t-0.0206067\t-0.183218\n",
      "-0.0135992\t1.13693\t0.0833942\t-1.34971\n",
      "0.0614744\t-0.455814\t1.5141\t4.30872\n",
      "0\t0\t0\t1\n",
      "[reg_aladin_sym] Final backward transformation matrix::\n",
      "0.93251\t0.00744401\t0.0122813\t0.127983\n",
      "0.0136302\t0.860665\t-0.0472186\t1.3676\n",
      "-0.0337579\t0.258798\t0.645745\t-2.43922\n",
      "0\t0\t0\t1\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "Registration Performed in 0 min 7 sec\n",
      "Have a good day !\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/fMRI/testDataSmoothBet.nii.gz -flo /testData/T2w/testDataBiasBet_Anno.nii.gz -cpp /testData/fMRI/testDataSmoothBettransMatrixAff.txt -inter 0 -res /testData/fMRI/testDataSmoothBet_Anno.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/fMRI/testDataSmoothBet.nii.gz\n",
      "\t128x128x20 voxels, 1 volumes\n",
      "\t0.140625x0.140625x0.4 mm\n",
      "Floating image name: /testData/T2w/testDataBiasBet_Anno.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/fMRI/testDataSmoothBet_Anno.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/T2w/testDataBiasBet_Anno.nii.gz -flo /aida/lib/ARA_annotationR+2000.nii.gz -trans /testData/T2w/testDataBiasBetMatrixBspline.nii -inter 0 -res /testData/fMRI/testDataSmoothBet_AnnoSplit.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/T2w/testDataBiasBet_Anno.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "Floating image name: /aida/lib/ARA_annotationR+2000.nii.gz\n",
      "\t228x160x264 voxels, 1 volumes\n",
      "\t0.05x0.05x0.05 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/fMRI/testDataSmoothBet_AnnoSplit.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/fMRI/testDataSmoothBet.nii.gz -flo /testData/fMRI/testDataSmoothBet_AnnoSplit.nii.gz -trans /testData/fMRI/testDataSmoothBettransMatrixAff.txt -inter 0 -res /testData/fMRI/testDataSmoothBet_AnnoSplit.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/fMRI/testDataSmoothBet.nii.gz\n",
      "\t128x128x20 voxels, 1 volumes\n",
      "\t0.140625x0.140625x0.4 mm\n",
      "Floating image name: /testData/fMRI/testDataSmoothBet_AnnoSplit.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/fMRI/testDataSmoothBet_AnnoSplit.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/T2w/testDataBiasBet_Anno.nii.gz -flo /aida/lib/annoVolume+2000_rsfMRI.nii.gz -trans /testData/T2w/testDataBiasBetMatrixBspline.nii -inter 0 -res /testData/fMRI/testDataSmoothBet_AnnoSplit_rsfMRI.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/T2w/testDataBiasBet_Anno.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "Floating image name: /aida/lib/annoVolume+2000_rsfMRI.nii.gz\n",
      "\t228x160x264 voxels, 1 volumes\n",
      "\t0.05x0.05x0.05 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/fMRI/testDataSmoothBet_AnnoSplit_rsfMRI.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/fMRI/testDataSmoothBet.nii.gz -flo /testData/fMRI/testDataSmoothBet_AnnoSplit_rsfMRI.nii.gz -trans /testData/fMRI/testDataSmoothBettransMatrixAff.txt -inter 0 -res /testData/fMRI/testDataSmoothBet_AnnoSplit_rsfMRI.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/fMRI/testDataSmoothBet.nii.gz\n",
      "\t128x128x20 voxels, 1 volumes\n",
      "\t0.140625x0.140625x0.4 mm\n",
      "Floating image name: /testData/fMRI/testDataSmoothBet_AnnoSplit_rsfMRI.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/fMRI/testDataSmoothBet_AnnoSplit_rsfMRI.nii.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/T2w/testDataBiasBet_Anno.nii.gz -flo /aida/lib/annoVolume.nii.gz -trans /testData/T2w/testDataBiasBetMatrixBspline.nii -inter 0 -res /testData/fMRI/testDataSmoothBet_Anno_rsfMRI.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/T2w/testDataBiasBet_Anno.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "Floating image name: /aida/lib/annoVolume.nii.gz\n",
      "\t228x160x264 voxels, 1 volumes\n",
      "\t0.05x0.05x0.05 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/fMRI/testDataSmoothBet_Anno_rsfMRI.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/fMRI/testDataSmoothBet.nii.gz -flo /testData/fMRI/testDataSmoothBet_Anno_rsfMRI.nii.gz -trans /testData/fMRI/testDataSmoothBettransMatrixAff.txt -inter 0 -res /testData/fMRI/testDataSmoothBet_Anno_rsfMRI.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/fMRI/testDataSmoothBet.nii.gz\n",
      "\t128x128x20 voxels, 1 volumes\n",
      "\t0.140625x0.140625x0.4 mm\n",
      "Floating image name: /testData/fMRI/testDataSmoothBet_Anno_rsfMRI.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/fMRI/testDataSmoothBet_Anno_rsfMRI.nii.gz\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "Command line:\n",
      " reg_resample -ref /testData/fMRI/testDataSmoothBet.nii.gz -flo /testData/T2w/testDataBiasBet_TemplateAllen.nii.gz -cpp /testData/fMRI/testDataSmoothBettransMatrixAff.txt -res /testData/fMRI/testDataSmoothBet_Template.nii.gz\n",
      "\n",
      "Parameters\n",
      "Reference image name: /testData/fMRI/testDataSmoothBet.nii.gz\n",
      "\t128x128x20 voxels, 1 volumes\n",
      "\t0.140625x0.140625x0.4 mm\n",
      "Floating image name: /testData/T2w/testDataBiasBet_TemplateAllen.nii.gz\n",
      "\t256x256x48 voxels, 1 volumes\n",
      "\t0.0683594x0.0683594x0.3 mm\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "[NiftyReg] Resampled image has been saved: /testData/fMRI/testDataSmoothBet_Template.nii.gz\n",
      "rsfMRI Registration  \u001b[0;30;42m COMPLETED \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /aida/bin/2.3_fMRIPreProcessing aidamri \\\n",
    "python registration_rsfMRI.py -i /testData/fMRI/testDataSmoothBet.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the contents. Again, the brain extracted registered brain image (rsfMRI) can be seen below.\n",
    "![fMRI registered brain image](images/itk_testData_fMRI_cropped.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registration.log\r\n",
      "testDataSmoothBet_Template.nii.gz\r\n",
      "testDataSmoothBet_Anno_rsfMRI.nii.gz\r\n",
      "testDataSmoothBet_AnnoSplit_rsfMRI.nii.gz\r\n",
      "testDataSmoothBet_AnnoSplit.nii.gz\r\n",
      "testDataSmoothBet_Anno.nii.gz\r\n",
      "testDataSmoothBet_T2w.nii.gz\r\n",
      "testDataSmoothBettransMatrixAff.txt\r\n",
      "preprocess.log\r\n",
      "testDataSmoothBet.nii.gz\r\n",
      "testDataSmoothBet_mask.nii.gz\r\n",
      "testDataSmooth.nii.gz\r\n",
      "testDataDN.nii.gz\r\n",
      "testData.6.1.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/fMRI aidamri ls -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may calculate the adjacency matrix using the physiology data provided from the test data set. If no physiology data is present, this step can be omitted. You do not need to invoke the physiology data directy. Instead, call the function in fMRI activity as shown below and use it on the initial fMRI data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRI Processing \u001b[5m...\u001b[0m (wait!)\r",
      "fMRI Processing  \u001b[0;30;42m COMPLETED \u001b[0m\r\n",
      "Regression \u001b[5m...\u001b[0m (wait!)\r",
      "Regression  \u001b[0;30;42m COMPLETED \u001b[0m\r\n",
      "sfrgr_file /testData/fMRI/regr/testData_mcf_f_st_SFRGR.nii.gz\r\n",
      "Copy Atlas Data and generate seed ROIs\r\n",
      "Output: /testData/fMRI/Seed_ROIs.nii.gz\r\n",
      "Copy Atlas Data and generate seed ROIs\r\n",
      "Output: /testData/fMRI/Seed_ROIs.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /aida/bin/3.3_fMRIActivity aidamri \\\n",
    "python process_fMRI.py -i /testData/fMRI/testData.6.1.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, check the contents of the folder again. Now, a regr folder should be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regr\r\n",
      "Seed_ROIs.nii.gz\r\n",
      "rs-fMRI_niiData\r\n",
      "process.log\r\n",
      "rawMonData\r\n",
      "rs-fMRI_mcf\r\n",
      "registration.log\r\n",
      "testDataSmoothBet_Template.nii.gz\r\n",
      "testDataSmoothBet_Anno_rsfMRI.nii.gz\r\n",
      "testDataSmoothBet_AnnoSplit_rsfMRI.nii.gz\r\n",
      "testDataSmoothBet_AnnoSplit.nii.gz\r\n",
      "testDataSmoothBet_Anno.nii.gz\r\n",
      "testDataSmoothBet_T2w.nii.gz\r\n",
      "testDataSmoothBettransMatrixAff.txt\r\n",
      "preprocess.log\r\n",
      "testDataSmoothBet.nii.gz\r\n",
      "testDataSmoothBet_mask.nii.gz\r\n",
      "testDataSmooth.nii.gz\r\n",
      "testDataDN.nii.gz\r\n",
      "testData.6.1.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -w /testData/fMRI aidamri ls -t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
